{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.3.0-py3-none-any.whl (144kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 2.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scipy in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scikit-learn in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.3.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.datasets import fetch_covtype # dataset\n",
    "from sklearn.model_selection import train_test_split # split dataset into training/test sets\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the dataset from:\n",
    "# \"http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "cover_type = fetch_covtype() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape:\n",
      " Counter({2: 283301, 1: 211840, 3: 35754, 7: 20510, 6: 17367, 5: 9493, 4: 2747})\n"
     ]
    }
   ],
   "source": [
    "# from the Forest_Cover_Type.ipynb data exploration we discovered there are 7 distinct cover_types\n",
    "# set these covertypes as our target, y \n",
    "y = cover_type.target\n",
    "\n",
    "print('Original dataset shape:\\n {}'.format(Counter(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 54)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data contains 54 features. Explored in depth within the Forest_Cover_Type.ipynb\n",
    "# set this 581012 x 54 matrix as our feature matrix, X\n",
    "X = cover_type.data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample w/ Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: \n",
      " Counter({1: 2747, 2: 2747, 3: 2747, 4: 2747, 5: 2747, 6: 2747, 7: 2747})\n"
     ]
    }
   ],
   "source": [
    "ros = RandomUnderSampler(random_state=42, return_indices=True, replacement=True)\n",
    "X_res, y_res, idx_resampled = ros.fit_sample(X, y)\n",
    "print('Resampled dataset shape: \\n {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reduce to 3.31% of the orignal dataset.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Reduce to {:.2f}% of the orignal dataset.\".format(X_res.shape[0]/X.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_types = { \n",
    "    \"2702\": \"Cathedral family - Rock outcrop complex, extremely stony.\",\n",
    "    \"2703\": \"Vanet - Ratake families complex, very stony.\", \n",
    "    \"2704\": \"Haploborolis - Rock outcrop complex, rubbly.\",\n",
    "    \"2705\": \"Ratake family - Rock outcrop complex, rubbly.\",\n",
    "    \"2706\": \"Vanet family - Rock outcrop complex complex, rubbly.\",\n",
    "    \"2717\": \"Vanet - Wetmore families - Rock outcrop complex, stony.\",\n",
    "    \"3501\": \"Gothic family.\",\n",
    "    \"3502\": \"Supervisor - Limber families complex.\",\n",
    "    \"4201\": \"Troutville family, very stony.\",\n",
    "    \"4703\": \"Bullwark - Catamount families - Rock outcrop complex, rubbly.\",\n",
    "    \"4704\": \"Bullwark - Catamount families - Rock land complex, rubbly.\",\n",
    "    \"4744\": \"Legault family - Rock land complex, stony.\",\n",
    "    \"4758\": \"Catamount family - Rock land - Bullwark family complex, rubbly.\",\n",
    "    \"5101\": \"Pachic Argiborolis - Aquolis complex.\",\n",
    "    \"5151\": \"not_in_survey\", # \"unspecified in the USFS Soil and ELU Survey.\",\n",
    "    \"6101\": \"Cryaquolis - Cryoborolis complex.\",\n",
    "    \"6102\": \"Gateview family - Cryaquolis complex.\",\n",
    "    \"6731\": \"Rogert family, very stony.\",\n",
    "    \"7101\": \"Typic Cryaquolis - Borohemists complex.\",\n",
    "    \"7102\": \"Typic Cryaquepts - Typic Cryaquolls complex.\",\n",
    "    \"7103\": \"Typic Cryaquolls - Leighcan family, till substratum complex.\",\n",
    "    \"7201\": \"Leighcan family, till substratum, extremely bouldery.\",\n",
    "    \"7202\": \"Leighcan family, till substratum - Typic Cryaquolls complex.\",\n",
    "    \"7700\": \"Leighcan family, extremely stony.\",\n",
    "    \"7701\": \"Leighcan family, warm, extremely stony.\",\n",
    "    \"7702\": \"Granile - Catamount families complex, very stony.\",\n",
    "    \"7709\": \"Leighcan family, warm - Rock outcrop complex, extremely stony.\",\n",
    "    \"7710\": \"Leighcan family - Rock outcrop complex, extremely stony.\",\n",
    "    \"7745\": \"Como - Legault families complex, extremely stony.\",\n",
    "    \"7746\": \"Como family - Rock land - Legault family complex, extremely stony.\",\n",
    "    \"7755\": \"Leighcan - Catamount families complex, extremely stony.\",\n",
    "    \"7756\": \"Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\",\n",
    "    \"7757\": \"Leighcan - Catamount families - Rock outcrop complex, extremely stony.\",\n",
    "    \"7790\": \"Cryorthents - Rock land complex, extremely stony.\",\n",
    "    \"8703\": \"Cryumbrepts - Rock outcrop - Cryaquepts complex.\",\n",
    "    \"8707\": \"Bross family - Rock land - Cryumbrepts complex, extremely stony.\",\n",
    "    \"8708\": \"Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\",\n",
    "    \"8771\": \"Leighcan - Moran families - Cryaquolls complex, extremely stony.\",\n",
    "    \"8772\": \"Moran family - Cryorthents - Leighcan family complex, extremely stony.\",\n",
    "    \"8776\": \"Moran family - Cryorthents - Rock land complex, extremely stony.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in soil_types.items():\n",
    "    fun = re.split(' - |, ', v.lower().replace(\".\", \"\"))\n",
    "    colocations = [i.replace(\" \", \"_\") for i in fun]\n",
    "    soil_types[k] = \" \".join(colocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2702': 'cathedral_family rock_outcrop_complex extremely_stony',\n",
       " '2703': 'vanet ratake_families_complex very_stony',\n",
       " '2704': 'haploborolis rock_outcrop_complex rubbly',\n",
       " '2705': 'ratake_family rock_outcrop_complex rubbly',\n",
       " '2706': 'vanet_family rock_outcrop_complex_complex rubbly',\n",
       " '2717': 'vanet wetmore_families rock_outcrop_complex stony',\n",
       " '3501': 'gothic_family',\n",
       " '3502': 'supervisor limber_families_complex',\n",
       " '4201': 'troutville_family very_stony',\n",
       " '4703': 'bullwark catamount_families rock_outcrop_complex rubbly',\n",
       " '4704': 'bullwark catamount_families rock_land_complex rubbly',\n",
       " '4744': 'legault_family rock_land_complex stony',\n",
       " '4758': 'catamount_family rock_land bullwark_family_complex rubbly',\n",
       " '5101': 'pachic_argiborolis aquolis_complex',\n",
       " '5151': 'not_in_survey',\n",
       " '6101': 'cryaquolis cryoborolis_complex',\n",
       " '6102': 'gateview_family cryaquolis_complex',\n",
       " '6731': 'rogert_family very_stony',\n",
       " '7101': 'typic_cryaquolis borohemists_complex',\n",
       " '7102': 'typic_cryaquepts typic_cryaquolls_complex',\n",
       " '7103': 'typic_cryaquolls leighcan_family till_substratum_complex',\n",
       " '7201': 'leighcan_family till_substratum extremely_bouldery',\n",
       " '7202': 'leighcan_family till_substratum typic_cryaquolls_complex',\n",
       " '7700': 'leighcan_family extremely_stony',\n",
       " '7701': 'leighcan_family warm extremely_stony',\n",
       " '7702': 'granile catamount_families_complex very_stony',\n",
       " '7709': 'leighcan_family warm rock_outcrop_complex extremely_stony',\n",
       " '7710': 'leighcan_family rock_outcrop_complex extremely_stony',\n",
       " '7745': 'como legault_families_complex extremely_stony',\n",
       " '7746': 'como_family rock_land legault_family_complex extremely_stony',\n",
       " '7755': 'leighcan catamount_families_complex extremely_stony',\n",
       " '7756': 'catamount_family rock_outcrop leighcan_family_complex extremely_stony',\n",
       " '7757': 'leighcan catamount_families rock_outcrop_complex extremely_stony',\n",
       " '7790': 'cryorthents rock_land_complex extremely_stony',\n",
       " '8703': 'cryumbrepts rock_outcrop cryaquepts_complex',\n",
       " '8707': 'bross_family rock_land cryumbrepts_complex extremely_stony',\n",
       " '8708': 'rock_outcrop cryumbrepts cryorthents_complex extremely_stony',\n",
       " '8771': 'leighcan moran_families cryaquolls_complex extremely_stony',\n",
       " '8772': 'moran_family cryorthents leighcan_family_complex extremely_stony',\n",
       " '8776': 'moran_family cryorthents rock_land_complex extremely_stony'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First digit:  climatic zone   \n",
    "first_digit = { \"1\": \"lower montane dry\",\n",
    "                \"2\": \"lower montane\",          \n",
    "                \"3\": \"montane dry\",            \n",
    "                \"4\": \"montane\",                \n",
    "                \"5\": \"montane dry and montane\",\n",
    "                \"6\": \"montane and subalpine\",\n",
    "                \"7\": \"subalpine\",  \n",
    "                \"8\": \"alpine\" \n",
    "              }  \n",
    "\n",
    "# Second digit:  geologic zones\n",
    "second_digit = {\"1\": \"alluvium\",\n",
    "                \"2\": \"glacial\",\n",
    "                \"3\": \"shale\",\n",
    "                \"4\": \"sandstone\",\n",
    "                \"5\": \"mixed sedimentary\",\n",
    "                \"6\": \"not_in_survey\", #\"unspecified in the USFS ELU Survey\"\n",
    "                \"7\": \"igneous and metamorphic\",\n",
    "                \"8\": \"volcanic\"\n",
    "               }\n",
    "\n",
    "# The third and fourth ELU digits are unique to the mapping unit and \n",
    "# have no special meaning to the climatic or geologic zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in first_digit.items():\n",
    "    first_digit[k] = v.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'lower_montane_dry',\n",
       " '2': 'lower_montane',\n",
       " '3': 'montane_dry',\n",
       " '4': 'montane',\n",
       " '5': 'montane_dry_and_montane',\n",
       " '6': 'montane_and_subalpine',\n",
       " '7': 'subalpine',\n",
       " '8': 'alpine'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in second_digit.items():\n",
    "    second_digit[k] = v.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'alluvium',\n",
       " '2': 'glacial',\n",
       " '3': 'shale',\n",
       " '4': 'sandstone',\n",
       " '5': 'mixed_sedimentary',\n",
       " '6': 'not_in_survey',\n",
       " '7': 'igneous_and_metamorphic',\n",
       " '8': 'volcanic'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_types_extend = defaultdict(str, soil_types)\n",
    "\n",
    "for k in soil_types_extend.keys():\n",
    "    climatic = \"climatic_zone_\" + first_digit.get(k[0])\n",
    "    geologic = \"geologic_zone_\" + second_digit.get(k[1])\n",
    "    soil_types_extend[k] += \" \" + climatic + \" \" + geologic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'2702': 'cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2703': 'vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2704': 'haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2705': 'ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2706': 'vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2717': 'vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '3501': 'gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary',\n",
       "             '3502': 'supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary',\n",
       "             '4201': 'troutville_family very_stony climatic_zone_montane geologic_zone_glacial',\n",
       "             '4703': 'bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4704': 'bullwark catamount_families rock_land_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4744': 'legault_family rock_land_complex stony climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4758': 'catamount_family rock_land bullwark_family_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '5101': 'pachic_argiborolis aquolis_complex climatic_zone_montane_dry_and_montane geologic_zone_alluvium',\n",
       "             '5151': 'not_in_survey climatic_zone_montane_dry_and_montane geologic_zone_alluvium',\n",
       "             '6101': 'cryaquolis cryoborolis_complex climatic_zone_montane_and_subalpine geologic_zone_alluvium',\n",
       "             '6102': 'gateview_family cryaquolis_complex climatic_zone_montane_and_subalpine geologic_zone_alluvium',\n",
       "             '6731': 'rogert_family very_stony climatic_zone_montane_and_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7101': 'typic_cryaquolis borohemists_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7102': 'typic_cryaquepts typic_cryaquolls_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7103': 'typic_cryaquolls leighcan_family till_substratum_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7201': 'leighcan_family till_substratum extremely_bouldery climatic_zone_subalpine geologic_zone_glacial',\n",
       "             '7202': 'leighcan_family till_substratum typic_cryaquolls_complex climatic_zone_subalpine geologic_zone_glacial',\n",
       "             '7700': 'leighcan_family extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7701': 'leighcan_family warm extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7702': 'granile catamount_families_complex very_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7709': 'leighcan_family warm rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7710': 'leighcan_family rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7745': 'como legault_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7746': 'como_family rock_land legault_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7755': 'leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7756': 'catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7757': 'leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7790': 'cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8703': 'cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8707': 'bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8708': 'rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8771': 'leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8772': 'moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8776': 'moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_types_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The wilderness areas are\n",
    "wilderness_areas =   {'Wilderness_Area1': \"Rawah Wilderness Area\", \n",
    "                      'Wilderness_Area2': \"Neota Wilderness Area\",\n",
    "                      'Wilderness_Area3': \"Comanche Peak Wilderness Area\",\n",
    "                      'Wilderness_Area4': \"Cache la Poudre Wilderness Area\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in wilderness_areas.items():\n",
    "    wilderness_areas[k] = v.lower().replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wilderness_Area1': 'rawah_wilderness_area',\n",
       " 'Wilderness_Area2': 'neota_wilderness_area',\n",
       " 'Wilderness_Area3': 'comanche_peak_wilderness_area',\n",
       " 'Wilderness_Area4': 'cache_la_poudre_wilderness_area'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilderness_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_cols = list(soil_types_extend.values())\n",
    "wilderness_cols = list(wilderness_areas.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "#        'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "#        'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "#        'Horizontal_Distance_To_Fire_Points'] \n",
    "# cols += wilderness_cols + soil_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawah_wilderness_area</th>\n",
       "      <th>neota_wilderness_area</th>\n",
       "      <th>comanche_peak_wilderness_area</th>\n",
       "      <th>cache_la_poudre_wilderness_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rawah_wilderness_area  neota_wilderness_area  \\\n",
       "0                    0.0                    0.0   \n",
       "1                    0.0                    0.0   \n",
       "2                    0.0                    0.0   \n",
       "3                    0.0                    0.0   \n",
       "4                    0.0                    0.0   \n",
       "\n",
       "   comanche_peak_wilderness_area  cache_la_poudre_wilderness_area  \n",
       "0                            1.0                              0.0  \n",
       "1                            1.0                              0.0  \n",
       "2                            1.0                              0.0  \n",
       "3                            1.0                              0.0  \n",
       "4                            1.0                              0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_test = pd.DataFrame(X_res[:, 10:14], columns=wilderness_cols).head()\n",
    "wild_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area\n",
       "1    comanche_peak_wilderness_area\n",
       "2    comanche_peak_wilderness_area\n",
       "3    comanche_peak_wilderness_area\n",
       "4    comanche_peak_wilderness_area\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(wild_test.idxmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary</th>\n",
       "      <th>supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary</th>\n",
       "      <th>troutville_family very_stony climatic_zone_montane geologic_zone_glacial</th>\n",
       "      <th>bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>...</th>\n",
       "      <th>leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                         \n",
       "1                                                0.0                                                                         \n",
       "2                                                0.0                                                                         \n",
       "3                                                0.0                                                                         \n",
       "4                                                0.0                                                                         \n",
       "\n",
       "   vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                            \n",
       "1                                                0.0                                                            \n",
       "2                                                0.0                                                            \n",
       "3                                                0.0                                                            \n",
       "4                                                0.0                                                            \n",
       "\n",
       "   haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                            \n",
       "1                                                0.0                                                            \n",
       "2                                                0.0                                                            \n",
       "3                                                0.0                                                            \n",
       "4                                                0.0                                                            \n",
       "\n",
       "   ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                             \n",
       "1                                                0.0                                                             \n",
       "2                                                0.0                                                             \n",
       "3                                                0.0                                                             \n",
       "4                                                0.0                                                             \n",
       "\n",
       "   vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                    \n",
       "1                                                0.0                                                                    \n",
       "2                                                0.0                                                                    \n",
       "3                                                0.0                                                                    \n",
       "4                                                0.0                                                                    \n",
       "\n",
       "   vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                     \n",
       "1                                                0.0                                                                     \n",
       "2                                                0.0                                                                     \n",
       "3                                                0.0                                                                     \n",
       "4                                                0.0                                                                     \n",
       "\n",
       "   gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary  \\\n",
       "0                                                0.0                         \n",
       "1                                                0.0                         \n",
       "2                                                0.0                         \n",
       "3                                                0.0                         \n",
       "4                                                0.0                         \n",
       "\n",
       "   supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary  \\\n",
       "0                                                0.0                                              \n",
       "1                                                0.0                                              \n",
       "2                                                0.0                                              \n",
       "3                                                0.0                                              \n",
       "4                                                0.0                                              \n",
       "\n",
       "   troutville_family very_stony climatic_zone_montane geologic_zone_glacial  \\\n",
       "0                                                0.0                          \n",
       "1                                                0.0                          \n",
       "2                                                0.0                          \n",
       "3                                                0.0                          \n",
       "4                                                0.0                          \n",
       "\n",
       "   bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                     \n",
       "1                                                0.0                                                                     \n",
       "2                                                0.0                                                                     \n",
       "3                                                0.0                                                                     \n",
       "4                                                0.0                                                                     \n",
       "\n",
       "                                                           ...                                                            \\\n",
       "0                                                          ...                                                             \n",
       "1                                                          ...                                                             \n",
       "2                                                          ...                                                             \n",
       "3                                                          ...                                                             \n",
       "4                                                          ...                                                             \n",
       "\n",
       "   leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                   \n",
       "1                                                0.0                                                                   \n",
       "2                                                1.0                                                                   \n",
       "3                                                0.0                                                                   \n",
       "4                                                0.0                                                                   \n",
       "\n",
       "   catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                1.0                                                                                     \n",
       "1                                                0.0                                                                                     \n",
       "2                                                0.0                                                                                     \n",
       "3                                                1.0                                                                                     \n",
       "4                                                0.0                                                                                     \n",
       "\n",
       "   leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                                \n",
       "1                                                0.0                                                                                \n",
       "2                                                0.0                                                                                \n",
       "3                                                0.0                                                                                \n",
       "4                                                0.0                                                                                \n",
       "\n",
       "   cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                             \n",
       "1                                                0.0                                                             \n",
       "2                                                0.0                                                             \n",
       "3                                                0.0                                                             \n",
       "4                                                0.0                                                             \n",
       "\n",
       "   cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                        \n",
       "1                                                0.0                                                        \n",
       "2                                                0.0                                                        \n",
       "3                                                0.0                                                        \n",
       "4                                                0.0                                                        \n",
       "\n",
       "   bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                       \n",
       "1                                                0.0                                                                       \n",
       "2                                                0.0                                                                       \n",
       "3                                                0.0                                                                       \n",
       "4                                                0.0                                                                       \n",
       "\n",
       "   rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                         \n",
       "1                                                0.0                                                                         \n",
       "2                                                0.0                                                                         \n",
       "3                                                0.0                                                                         \n",
       "4                                                0.0                                                                         \n",
       "\n",
       "   leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                       \n",
       "1                                                0.0                                                                       \n",
       "2                                                0.0                                                                       \n",
       "3                                                0.0                                                                       \n",
       "4                                                0.0                                                                       \n",
       "\n",
       "   moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                             \n",
       "1                                                0.0                                                                             \n",
       "2                                                0.0                                                                             \n",
       "3                                                0.0                                                                             \n",
       "4                                                0.0                                                                             \n",
       "\n",
       "   moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \n",
       "0                                                0.0                                                                      \n",
       "1                                                1.0                                                                      \n",
       "2                                                0.0                                                                      \n",
       "3                                                0.0                                                                      \n",
       "4                                                0.0                                                                      \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_test = pd.DataFrame(X_res[:, 14:], columns=soil_cols).head()\n",
    "soil_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area. catamount_famil...\n",
       "1    comanche_peak_wilderness_area. moran_family cr...\n",
       "2    comanche_peak_wilderness_area. leighcan catamo...\n",
       "3    comanche_peak_wilderness_area. catamount_famil...\n",
       "4    comanche_peak_wilderness_area. leighcan_family...\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_test.idxmax(axis=1) + \". \" + soil_test.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area catamount_family...\n",
       "1    comanche_peak_wilderness_area moran_family cry...\n",
       "2    comanche_peak_wilderness_area leighcan catamou...\n",
       "3    comanche_peak_wilderness_area catamount_family...\n",
       "4    comanche_peak_wilderness_area leighcan_family ...\n",
       "5    comanche_peak_wilderness_area catamount_family...\n",
       "6    neota_wilderness_area catamount_family rock_ou...\n",
       "7    rawah_wilderness_area como legault_families_co...\n",
       "8    rawah_wilderness_area leighcan_family till_sub...\n",
       "9    comanche_peak_wilderness_area catamount_family...\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_df = pd.DataFrame(X_res[:, 10:14], columns=wilderness_cols)\n",
    "\n",
    "soil_df = pd.DataFrame(X_res[:, 14:], columns=soil_cols)\n",
    "\n",
    "X_wild_soil = wild_df.idxmax(axis=1) + \" \" + soil_df.idxmax(axis=1)\n",
    "X_wild_soil.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comanche_peak_wilderness_area catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wild_soil[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 75), set())"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer for entire dataset\n",
    "min_df = 1 # default making a point to keep all features if min_df=2 then only token removed is 'not_in_survey'\n",
    "max_df = 0.95 # unless they appear in all docs \n",
    "max_features = 100\n",
    "vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "\n",
    "X_vectorized = vectorizer.fit_transform(X_wild_soil)\n",
    "X_vectorized.shape, vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectorized, y_res, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5748829953198128"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 7, 4, 2], dtype=int32)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57678232006156116"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.49889625,  0.42349727,  0.52826087,  0.6572327 ,  0.62823529,\n",
       "         0.48854962,  0.88041237]),\n",
       " array([ 0.41620626,  0.55956679,  0.42706503,  0.77839851,  0.48545455,\n",
       "         0.56338028,  0.81333333]),\n",
       " array([ 0.45381526,  0.48211509,  0.47230321,  0.71270247,  0.54769231,\n",
       "         0.52330335,  0.84554455]),\n",
       " array([543, 554, 569, 537, 550, 568, 525]))"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_recall_fscore_support(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.42      0.45       543\n",
      "          2       0.42      0.56      0.48       554\n",
      "          3       0.53      0.43      0.47       569\n",
      "          4       0.66      0.78      0.71       537\n",
      "          5       0.63      0.49      0.55       550\n",
      "          6       0.49      0.56      0.52       568\n",
      "          7       0.88      0.81      0.85       525\n",
      "\n",
      "avg / total       0.58      0.57      0.57      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[226, 227,   1,   0,  35,   4,  50],\n",
       "       [108, 310,   7,   3,  81,  37,   8],\n",
       "       [  0,   1, 243, 135,   3, 187,   0],\n",
       "       [  0,   0,  71, 418,   0,  48,   0],\n",
       "       [ 71, 110,  43,   0, 267,  59,   0],\n",
       "       [ 15,  24,  95,  80,  34, 320,   0],\n",
       "       [ 33,  60,   0,   0,   5,   0, 427]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages/sklearn/naive_bayes.py:472: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57514300572022881"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test) # accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.feature_log_prob_[0]<-32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 7, 4, 2], dtype=int32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57703767801433958"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.42      0.45       543\n",
      "          2       0.42      0.56      0.48       554\n",
      "          3       0.53      0.43      0.47       569\n",
      "          4       0.66      0.78      0.71       537\n",
      "          5       0.63      0.49      0.55       550\n",
      "          6       0.49      0.56      0.52       568\n",
      "          7       0.88      0.81      0.85       525\n",
      "\n",
      "avg / total       0.58      0.58      0.57      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 75), set())"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidf on entire dataset (balanced)\n",
    "min_df = 1 # default making a point to keep all features \n",
    "max_df = 0.95 # unless they appear in all docs \n",
    "max_features = 100 # not a problem here... soil + wilderness has a max of 70 feats\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "\n",
    "X_tfidf = tfidf_vec.fit_transform(X_wild_soil)\n",
    "X_tfidf.shape, tfidf_vec.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_tfidf, y_res, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57384295371814875"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha=0.1)\n",
    "clf.fit(X_train2, y_train2)\n",
    "clf.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, array([  4.46240705e-06,   1.83518055e-03,   7.03904301e-01,\n",
       "          2.38418404e-02,   3.58901009e-02,   2.34522845e-01,\n",
       "          1.26992217e-06]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2 = clf.predict(X_test2)\n",
    "y_probs2 = clf.predict_proba(X_test2)\n",
    "y_pred2[1], y_probs2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.50      0.42      0.46       543\n",
      "          2       0.42      0.55      0.48       554\n",
      "          3       0.50      0.51      0.50       569\n",
      "          4       0.74      0.65      0.69       537\n",
      "          5       0.63      0.48      0.55       550\n",
      "          6       0.49      0.60      0.54       568\n",
      "          7       0.88      0.81      0.85       525\n",
      "\n",
      "avg / total       0.59      0.57      0.58      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test2, y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method XGBModel.get_params of XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [50, 150],#np.arange(50, 400, 50),\n",
    "              \"max_depth\": [4, 5], #np.arange(1, 11, 2), \n",
    "              \"learning_rate\": [0.05, 0.01]#[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "             }\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=5)\n",
    "results = grid_search.fit(X_tfidf, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>mean_validation_score</th>\n",
       "      <th>cv_validation_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.588746</td>\n",
       "      <td>[0.587272727273, 0.583376623377, 0.59276606817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.593271</td>\n",
       "      <td>[0.592467532468, 0.586233766234, 0.59562841530...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.590930</td>\n",
       "      <td>[0.587792207792, 0.586233766234, 0.59432734842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'learning_rate': 0.05, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.593323</td>\n",
       "      <td>[0.592467532468, 0.586233766234, 0.59588862867...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.582142</td>\n",
       "      <td>[0.576883116883, 0.575324675325, 0.58417902680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 4, 'n_est...</td>\n",
       "      <td>0.587134</td>\n",
       "      <td>[0.585714285714, 0.581558441558, 0.59198542805...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.587966</td>\n",
       "      <td>[0.584935064935, 0.582597402597, 0.59276606817...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.590202</td>\n",
       "      <td>[0.587792207792, 0.584155844156, 0.59276606817...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          parameters  mean_validation_score  \\\n",
       "0  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...               0.588746   \n",
       "1  {'learning_rate': 0.05, 'max_depth': 4, 'n_est...               0.593271   \n",
       "2  {'learning_rate': 0.05, 'max_depth': 5, 'n_est...               0.590930   \n",
       "3  {'learning_rate': 0.05, 'max_depth': 5, 'n_est...               0.593323   \n",
       "4  {'learning_rate': 0.01, 'max_depth': 4, 'n_est...               0.582142   \n",
       "5  {'learning_rate': 0.01, 'max_depth': 4, 'n_est...               0.587134   \n",
       "6  {'learning_rate': 0.01, 'max_depth': 5, 'n_est...               0.587966   \n",
       "7  {'learning_rate': 0.01, 'max_depth': 5, 'n_est...               0.590202   \n",
       "\n",
       "                                cv_validation_scores  \n",
       "0  [0.587272727273, 0.583376623377, 0.59276606817...  \n",
       "1  [0.592467532468, 0.586233766234, 0.59562841530...  \n",
       "2  [0.587792207792, 0.586233766234, 0.59432734842...  \n",
       "3  [0.592467532468, 0.586233766234, 0.59588862867...  \n",
       "4  [0.576883116883, 0.575324675325, 0.58417902680...  \n",
       "5  [0.585714285714, 0.581558441558, 0.59198542805...  \n",
       "6  [0.584935064935, 0.582597402597, 0.59276606817...  \n",
       "7  [0.587792207792, 0.584155844156, 0.59276606817...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results.grid_scores_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: -0.935569% using {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f%% using %s\" % (results.best_score_, results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_estimators\": [50, 150],#np.arange(50, 400, 50),\n",
    "              \"max_depth\": [4, 5], #np.arange(1, 11, 2), \n",
    "              \"learning_rate\": [0.05, 0.01]#[0.0001, 0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "             }\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, scoring=\"accuracy\", n_jobs=-1, cv=5)\n",
    "results = grid_search.fit(X_tfidf, y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 59.332259% using {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best: %f%% using %s\" % (results.best_score_ *100, results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X_tfidf, y_res, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 51)\t0.559307168682\n",
      "  (0, 70)\t0.458806793127\n",
      "  (0, 11)\t0.320399533591\n",
      "  (0, 72)\t0.537955900499\n",
      "  (0, 34)\t0.153154148469\n",
      "  (0, 17)\t0.247311973915\n"
     ]
    }
   ],
   "source": [
    "print(X_train3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49579182,  0.28611025,  0.00252211,  0.00071815,  0.04380472,\n",
       "         0.10483235,  0.06622061]], dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.predict_proba(X_test3)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred3[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59516380655226209"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.score(X_test3, y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.51      0.40      0.45       543\n",
      "          2       0.43      0.59      0.50       554\n",
      "          3       0.52      0.51      0.52       569\n",
      "          4       0.77      0.71      0.74       537\n",
      "          5       0.61      0.56      0.58       550\n",
      "          6       0.54      0.60      0.57       568\n",
      "          7       0.89      0.81      0.85       525\n",
      "\n",
      "avg / total       0.61      0.60      0.60      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = xgb_clf.predict(X_test3)\n",
    "print(classification_report(y_test3, y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_clf2 = XGBClassifier(**{'n_estimators': 350, 'learning_rate': 0.3, 'max_depth': 9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.3, max_delta_step=0,\n",
       "       max_depth=9, min_child_weight=1, missing=None, n_estimators=350,\n",
       "       n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf2.fit(X_test3, y_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32085283411336452"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_3 = xgb_clf2.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.49      0.45      0.47       543\n",
      "          2       0.47      0.50      0.49       554\n",
      "          3       0.59      0.40      0.48       569\n",
      "          4       0.68      0.83      0.75       537\n",
      "          5       0.58      0.62      0.60       550\n",
      "          6       0.54      0.60      0.57       568\n",
      "          7       0.89      0.81      0.85       525\n",
      "\n",
      "avg / total       0.60      0.60      0.60      3846\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test3, y_pred_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3846, 7)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_3 = xgb_clf2.predict_proba(X_test3)\n",
    "y_pred_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 6, ..., 5, 4, 1], dtype=int32)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = y_test3 -1\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3846, 7)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M, N = y_pred_3.shape\n",
    "M, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.zeros([M, N])\n",
    "Y[np.arange(M), new] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4, 6, ..., 5, 4, 1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(Y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 5, 7, ..., 6, 5, 2], dtype=int32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3613527688662856e-15"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test3, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91418150018350819"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test3, y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.39549251e-03,   1.90627221e-02,   0.00000000e+00,\n",
       "         2.93532163e-02,   8.13117810e-03,   8.29987526e-02,\n",
       "         1.32932961e-02,   1.27197271e-02,   1.83541961e-02,\n",
       "         5.93812205e-03,   4.72688004e-02,   6.80859685e-02,\n",
       "         1.32932961e-02,   3.13775763e-02,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.72306761e-01,   9.18047205e-02,\n",
       "         1.31583388e-03,   2.22679577e-03,   0.00000000e+00,\n",
       "         2.29427451e-03,   2.69914628e-04,   1.29221631e-02,\n",
       "         0.00000000e+00,   3.18836682e-02,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   2.05809902e-03,\n",
       "         4.97317724e-02,   0.00000000e+00,   4.78761084e-02,\n",
       "         5.46577154e-03,   5.98873124e-02,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   5.06089942e-04,   0.00000000e+00,\n",
       "         3.91376205e-03,   3.27271484e-02,   1.31583388e-03,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.74094941e-02,   0.00000000e+00,   0.00000000e+00,\n",
       "         2.42923177e-03,   1.19437231e-02,   2.88808662e-02,\n",
       "         3.37393285e-04,   9.48075205e-03,   1.01217993e-04,\n",
       "         3.44815962e-02,   0.00000000e+00,   3.37393285e-05,\n",
       "         1.21124191e-02,   3.91376205e-03,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         6.74786570e-04,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.18087651e-03,   6.41047256e-04,   0.00000000e+00,\n",
       "         5.60072856e-03,   0.00000000e+00,   0.00000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf2.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# fitting our training data with scikit learn's RandomForestClassifier\n",
    "# Choosing this algorithm over others due to our feature space. \n",
    "# 44 of our features are binary, whether or not the tree is in one of 4 wilderness areas\n",
    "# and whether or not the tree is found in one of 40 soil types\n",
    "# In the Forest_Cover_Type.ipynb, the goal was to correctly classify only one tree type (7)\n",
    "# Through data exploration, and the logit function it was found that nearly all of the \n",
    "# features were statistically significant. \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.1,  0. ,  0. ,  0.9,  0. ,  0. ],\n",
       "       [ 0. ,  0.1,  0. ,  0. ,  0.9,  0. ,  0. ],\n",
       "       [ 0.1,  0.9,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = clf.predict_proba(X)[:5]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, 2, 5], dtype=int32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.063216309394701575"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y[:5], a, labels=np.arange(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9441155209803449"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using only 10 trees, we are able to predict with a very high accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=95787995, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srted_index():\n",
    "    idx_lst = []\n",
    "    for i, feat in enumerate(clf.feature_importances_):\n",
    "        idx_lst.append([feat, i])\n",
    "    return sorted(idx_lst, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.24660715000309424, 0],\n",
       " [0.11880706413405626, 5],\n",
       " [0.11152332641382312, 9],\n",
       " [0.059908103854401686, 3],\n",
       " [0.05645230632134722, 4],\n",
       " [0.047170463008234624, 1],\n",
       " [0.041989991641566075, 7],\n",
       " [0.041393662003483399, 6],\n",
       " [0.040573691144154225, 8],\n",
       " [0.03374382486116325, 2],\n",
       " [0.022282472125462558, 13],\n",
       " [0.016397530318239713, 35],\n",
       " [0.013449267507324187, 17],\n",
       " [0.013250193937924695, 23],\n",
       " [0.011661033568579889, 12],\n",
       " [0.010540464318108169, 15],\n",
       " [0.010170187339738216, 36],\n",
       " [0.010121195893610394, 25],\n",
       " [0.0096949731577236822, 10],\n",
       " [0.0091872477096017341, 52],\n",
       " [0.0089744993140728011, 51],\n",
       " [0.006788750888043503, 11],\n",
       " [0.0055391919733450423, 45],\n",
       " [0.0054293017962487528, 53],\n",
       " [0.0053588437574285715, 19],\n",
       " [0.0049212520631274469, 42],\n",
       " [0.0047951413589761554, 46],\n",
       " [0.0041894587820109786, 26],\n",
       " [0.0040481499729057464, 37],\n",
       " [0.0040246374942260969, 44],\n",
       " [0.0031794358396381893, 43],\n",
       " [0.0022974648204542028, 16],\n",
       " [0.0020622543592601233, 24],\n",
       " [0.0019328281663092354, 48],\n",
       " [0.0018229078709950189, 33],\n",
       " [0.0016653287563386, 30],\n",
       " [0.0016444338163877404, 14],\n",
       " [0.00087358872976643306, 32],\n",
       " [0.00086695437039335083, 29],\n",
       " [0.00073455187202405708, 34],\n",
       " [0.00064538854267874981, 40],\n",
       " [0.00063984919724873628, 50],\n",
       " [0.00061868426918536409, 18],\n",
       " [0.00056286891815842874, 47],\n",
       " [0.00034229307678299877, 39],\n",
       " [0.00031240009684501021, 27],\n",
       " [0.0002641289528231494, 38],\n",
       " [0.00016403733873380822, 41],\n",
       " [0.00012673478571201076, 49],\n",
       " [0.00010551968848467165, 31],\n",
       " [9.3768488960160678e-05, 22],\n",
       " [3.5761473959934998e-05, 21],\n",
       " [1.335644975974367e-05, 20],\n",
       " [2.0834570778478678e-06, 28]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf100 = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=1)\n",
    "clf100.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95633541014078693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No surprise, more trees, more accurate \n",
    "clf100.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  25.4s\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.5s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.3s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  33.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 4.1min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  26.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.1s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.8min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  32.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.8s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.2s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  22.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  24.7s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  23.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  44.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.8s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  37.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 15.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  36.4s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.2min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.4min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  32.6s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  28.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  26.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  29.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.3s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.8s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.2s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.4s\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 24.8min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_depth=np.arange(2,10),\n",
    "                  max_features=np.arange(7, 14),\n",
    "                  max_leaf_nodes=np.arange(2,8)\n",
    "                 )\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=13, max_leaf_nodes=7,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 13, 'max_leaf_nodes': 7, 'n_estimators': 100}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67040599720793248"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.1s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.3s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.2s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.1s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.0s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 5.8min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 6.4min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 6.0min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  57.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  55.2s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  50.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  16.9s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  17.4s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.1s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  14.0s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=  10.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.4s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.3s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 17.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. n_estimators=25, max_features=13, total=  16.0s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  15.6s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.2s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  18.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.4s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.2s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  12.9s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.2s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.1s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  21.5s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.5min\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.5s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  18.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 19.0min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_features=np.arange(7, 14)\n",
    "                 )\n",
    "\n",
    "grid_stump = RandomizedSearchCV(estimator=RandomForestClassifier(max_depth=1),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid_stump = grid_stump.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=1, max_features=13, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 13, 'n_estimators': 100}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59520758830391463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "                                 # use in the _make_split method\n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "\n",
    "\n",
    "        # This piece of code is used to provide feature names to the Decision tree\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            # if the user has not provided feature names, just give them numbers\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            # otherwise, these are the names\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # * Create True/False array of whether the variable is categorical\n",
    "        # use a lambda function called is_categorical to determine if the variable is an instance\n",
    "        # of str, bool or unicode - in that case is_categorical will be true\n",
    "        # otherwise False. Look up the function isinstance()\n",
    "\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) \n",
    "            \n",
    "        # Each variable (organized by index) is given a label categorical or not\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        # Call the build_tree function\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "\n",
    "        #  * initialize a root TreeNode\n",
    "        node = TreeNode()\n",
    "        # * set index, value, splits as the output of self._choose_split_index(X,y)\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        # if no index is returned from the split index or we cannot split\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            # * set the node to be a leaf\n",
    "            node.leaf = True\n",
    "            # * set the classes attribute to the number of classes\n",
    "            # * we have in this leaf with Counter()\n",
    "            node.classes = Counter(y)\n",
    "            # * set the name of the node to be the most common class in it\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "\n",
    "        else: # otherwise we can split (again this comes out of choose_split_index\n",
    "            # * set X1, y1, X2, y2 to be the splits\n",
    "            X1, y1, X2, y2 = splits\n",
    "            # * the node column should be set to the index coming from split_index\n",
    "            node.column = index\n",
    "            # * the node name is the feature name as determined by\n",
    "            #   the index (column name)\n",
    "            node.name = self.feature_names[index]\n",
    "\n",
    "            # * set the node value to be the value of the split\n",
    "            node.value = value\n",
    "\n",
    "            # * set the categorical flag of the node to be the category of the column\n",
    "            node.categorical = self.categorical[index]\n",
    "\n",
    "            # * now continue recursing down both branches of the split\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the total length of y. This is the p(C)\n",
    "            # * add the entropy p(C) ln p(C) to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C * np.log(p_C)\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the size of y. This is the p(C)\n",
    "            # * add p(C)**2 to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C**2\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "\n",
    "        # * slice the split column from X with the split_index\n",
    "        split_column = X[:, split_index]\n",
    "        # * if the variable of this column is categorical\n",
    "        if self.categorical[split_index]:\n",
    "            # * select the indices of the rows in the column\n",
    "            #  with the split_value (T/F) into one set of indices (call them A)\n",
    "            A = split_column == split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            # that don't have the split_value into another\n",
    "            #  set of indices (call them B)\n",
    "            B = split_column != split_value\n",
    "        # * else if the variable is not categorical\n",
    "        else:\n",
    "             # * select the indices of the rows in the column\n",
    "            #  less than the split value into one set of indices (call them A)\n",
    "            A = split_column < split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            #  greater or equal to  the split value into\n",
    "            # another set of indices (call them B)\n",
    "            B = split_column >= split_value\n",
    "            \n",
    "        return X[A], y[A], X[B], y[B]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "        # * set total equal to the impurity_criterion\n",
    "        total = self.impurity_criterion(y)\n",
    "        \n",
    "        e2 = len(y1)/len(y)*self.impurity_criterion(y1) + len(y2)/len(y)*self.impurity_criterion(y2)\n",
    "        total -= e2\n",
    "#         # * for each of the possible splits y1 and y2\n",
    "#         for split in  \n",
    "#             # * calculate the impurity_criterion of the split\n",
    "#             imp_cri = self.impurity_criterion(split) \n",
    "#             # * subtract this value from the total, multiplied by split_size/y_size\n",
    "#             total -= imp_cri * len(split)/\n",
    "            \n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "        X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "\n",
    "        # set these initial variables to None\n",
    "        split_index, split_value, split = None, None, None\n",
    "        # we need to keep track of the maximum entropic gain\n",
    "        max_gain = 0\n",
    "\n",
    "        # * for each column in X\n",
    "        for col in range(X.shape[1]):\n",
    "            # * set an array called values to be the\n",
    "            # unique values in that column (use np.unique)\n",
    "            values = np.unique(X[:, col])\n",
    "            # if there are less than 2 values, move on to the next column\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "\n",
    "            # * for each value V in the values array\n",
    "            for val in values:\n",
    "                # * make a temporary split (using the column index and V) with make_split\n",
    "                temporary_split = self._make_split(X, y, col, val)\n",
    "                # * calculate the information gain between the original y, y1 and y2\n",
    "                X1, y1, X2, y2 = temporary_split\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                # * if this gain is greater than the max_gain\n",
    "                if gain > max_gain:\n",
    "\n",
    "                    # * set max_gain, split_index, and split_value to be equal\n",
    "                    # to the current max_gain, column and value\n",
    "                    # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "                    split = temporary_split\n",
    "                    max_gain, split_index, split_value = gain, col, val\n",
    "                   \n",
    "        return split_index, split_value, split \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree. This will allow you to $:print tree\n",
    "        '''\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    '''A Random Forest class'''\n",
    "\n",
    "    def __init__(self, num_trees, num_features):\n",
    "        '''\n",
    "           num_trees:  number of trees to create in the forest:\n",
    "        num_features:  the number of features to consider when choosing the\n",
    "                           best split for each node of the decision trees\n",
    "        '''\n",
    "        self.num_trees = num_trees\n",
    "        self.num_features = num_features\n",
    "        self.forest = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X:  two dimensional numpy array representing feature matrix\n",
    "                for test data\n",
    "        y:  numpy array representing labels for test data\n",
    "        '''\n",
    "        self.forest = self.build_forest(X, y, self.num_trees, X.shape[0], \\\n",
    "                                        self.num_features)\n",
    "\n",
    "    def build_forest(self, X, y, num_trees, num_samples, num_features):\n",
    "\n",
    "        # * Return a list of num_trees DecisionTrees.\n",
    "        row, col = X.shape\n",
    "        forest = []\n",
    "        \n",
    "        for tree in range(num_trees):\n",
    "            # create a random set of X_samples with replacement\n",
    "            X_samp = np.random.randint(row, size=num_samples)\n",
    "            # create a random permutation of features (list)[sliced to length num_features]\n",
    "            y_samp = np.random.permutation(col)[:num_features]\n",
    "            X_tree = X[X_samp,:][:,y_samp]\n",
    "            y_tree = y[X_samp]\n",
    "            tree = DecisionTree()\n",
    "            tree.fit(X_tree, y_tree, feature_names=y_samp)\n",
    "            forest.append(tree)\n",
    "            \n",
    "        return forest\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        '''\n",
    "        Return a numpy array of the labels predicted for the given test data.\n",
    "        '''\n",
    "       \n",
    "        # * Each one of the trees is allowed to predict on the same row of input data. The majority vote\n",
    "        # is the output of the whole forest. This becomes a single prediction.\n",
    "        \n",
    "        predictions  = []\n",
    "        for tree in self.forest:\n",
    "            predict = tree.predict(X[:,tree.feature_names])\n",
    "            predictions.append(predict)\n",
    "        # thank you Tristan! Count along the columns and find the most common    \n",
    "        return np.array(np.apply_along_axis(lambda col: Counter(col).most_common()[0][0], arr=predictions, axis=0))\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "\n",
    "        '''\n",
    "        Return the accuracy of the Random Forest for the given test data.\n",
    "        '''\n",
    "\n",
    "        # * In this case you simply compute the accuracy formula as we have defined in class. Compare predicted y to\n",
    "        # the actual input y.\n",
    "        \n",
    "        return sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
