{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype # dataset\n",
    "from sklearn.model_selection import train_test_split # split dataset into training/test sets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the dataset from:\n",
    "# \"http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "cover_type = fetch_covtype() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = cover_type.target\n",
    "X = cover_type.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since our dataset is rather large, and we will be doing cross validation on our training set,\n",
    "# we set the train_size parameter to be 90% and set aside %10 to test on\n",
    "# due to large class imbalances in our target matrix, we\n",
    "# set the stratify parameter=y. this makes a split so that the proportion of classes in the \n",
    "# test and train sets will be similar\n",
    "X_train, X_test, y_train, y_test=\\\n",
    "                            train_test_split(X, y, train_size=.90, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4541668100925958"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_stump = AdaBoostClassifier()\n",
    "clf_stump.fit(X_train, y_train)\n",
    "clf_stump.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(algorithm='SAMME.R',\n",
       "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'),\n",
       "          learning_rate=1.0, n_estimators=50, random_state=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94127568758390412"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the default for AdaBoost Classifier is a Decision Stump with max_depth=1\n",
    "# when we take this default away, the decision tree continues splitting at each node until\n",
    "# it reaches purity, no surprise the model with an perfectly fit tree preforms better \n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Boost Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Given: \n",
    "    N, (number of samples)\n",
    "    estimators, (number of estimators)\n",
    "    weakL, (weak learner)\n",
    "    X_train, y_train (training set from data (X,y))\n",
    "\n",
    "# Initialize weights\n",
    "For each sample in range(N):\n",
    "    weight_sample = 1/N\n",
    "\n",
    "For each est in range(estimators):\n",
    "    # fit a weak learner to the training data using the weight \n",
    "    est = weakL.fit(X_train, y_train, weight)\n",
    "    # calculate error\n",
    "    error = sum(weight * (est(x) != y)) / sum(weight)\n",
    "    # calculate alpha\n",
    "    alpha = (1/2)ln((1-error)/error)\n",
    "    # Update weights\n",
    "    weight *= exp(alpha * (est(x) != y))\n",
    "    \n",
    "    For each i in range(N):\n",
    "        # estimator misclassifies y\n",
    "        if est(x_i) != y: \n",
    "            # increase weight of i for next estimator\n",
    "            weight_i(est + 1) = weight_i(est)/(2*error)\n",
    "        # estimator classifies y correctly\n",
    "        else:\n",
    "            # decrease weight of i for next estimator\n",
    "            weight_i(est + 1) = weight_i(est)/(2*(1-error))\n",
    "            \n",
    "# return sum of predictions for all est*alpha\n",
    "return sum(est*alpha)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reasons to use Ada Boost\n",
    "- Besides deciding on the number of weak classifiers (assumption is made) and the initial weight, there are no parameters to tune\n",
    "- Fast and versatile \n",
    "- You can have little prior knowledge about the weak classifier (for example, decision stumps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
