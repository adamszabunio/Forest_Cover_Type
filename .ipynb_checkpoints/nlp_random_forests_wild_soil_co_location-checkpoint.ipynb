{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced_learn-0.3.0-py3-none-any.whl (144kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 2.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scipy in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scikit-learn in /Users/adamszabunio/anaconda/envs/nlp/lib/python3.6/site-packages (from imbalanced-learn->imblearn)\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.3.0 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.datasets import fetch_covtype # dataset\n",
    "from sklearn.model_selection import train_test_split # split dataset into training/test sets\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the dataset from:\n",
    "# \"http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "cover_type = fetch_covtype() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape:\n",
      " Counter({2: 283301, 1: 211840, 3: 35754, 7: 20510, 6: 17367, 5: 9493, 4: 2747})\n"
     ]
    }
   ],
   "source": [
    "# from the Forest_Cover_Type.ipynb data exploration we discovered there are 7 distinct cover_types\n",
    "# set these covertypes as our target, y \n",
    "y = cover_type.target\n",
    "\n",
    "print('Original dataset shape:\\n {}'.format(Counter(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 54)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Our data contains 54 features. Explored in depth within the Forest_Cover_Type.ipynb\n",
    "# set this 581012 x 54 matrix as our feature matrix, X\n",
    "X = cover_type.data\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersample w/ Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: \n",
      " Counter({1: 2747, 2: 2747, 3: 2747, 4: 2747, 5: 2747, 6: 2747, 7: 2747})\n"
     ]
    }
   ],
   "source": [
    "ros = RandomUnderSampler(random_state=42, return_indices=True, replacement=True)\n",
    "X_res, y_res, idx_resampled = ros.fit_sample(X, y)\n",
    "print('Resampled dataset shape: \\n {}'.format(Counter(y_res)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reduce to 3.31% of the orignal dataset.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Reduce to {:.2f}% of the orignal dataset.\".format(X_res.shape[0]/X.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_types = { \n",
    "    \"2702\": \"Cathedral family - Rock outcrop complex, extremely stony.\",\n",
    "    \"2703\": \"Vanet - Ratake families complex, very stony.\", \n",
    "    \"2704\": \"Haploborolis - Rock outcrop complex, rubbly.\",\n",
    "    \"2705\": \"Ratake family - Rock outcrop complex, rubbly.\",\n",
    "    \"2706\": \"Vanet family - Rock outcrop complex complex, rubbly.\",\n",
    "    \"2717\": \"Vanet - Wetmore families - Rock outcrop complex, stony.\",\n",
    "    \"3501\": \"Gothic family.\",\n",
    "    \"3502\": \"Supervisor - Limber families complex.\",\n",
    "    \"4201\": \"Troutville family, very stony.\",\n",
    "    \"4703\": \"Bullwark - Catamount families - Rock outcrop complex, rubbly.\",\n",
    "    \"4704\": \"Bullwark - Catamount families - Rock land complex, rubbly.\",\n",
    "    \"4744\": \"Legault family - Rock land complex, stony.\",\n",
    "    \"4758\": \"Catamount family - Rock land - Bullwark family complex, rubbly.\",\n",
    "    \"5101\": \"Pachic Argiborolis - Aquolis complex.\",\n",
    "    \"5151\": \"not_in_survey\", # \"unspecified in the USFS Soil and ELU Survey.\",\n",
    "    \"6101\": \"Cryaquolis - Cryoborolis complex.\",\n",
    "    \"6102\": \"Gateview family - Cryaquolis complex.\",\n",
    "    \"6731\": \"Rogert family, very stony.\",\n",
    "    \"7101\": \"Typic Cryaquolis - Borohemists complex.\",\n",
    "    \"7102\": \"Typic Cryaquepts - Typic Cryaquolls complex.\",\n",
    "    \"7103\": \"Typic Cryaquolls - Leighcan family, till substratum complex.\",\n",
    "    \"7201\": \"Leighcan family, till substratum, extremely bouldery.\",\n",
    "    \"7202\": \"Leighcan family, till substratum - Typic Cryaquolls complex.\",\n",
    "    \"7700\": \"Leighcan family, extremely stony.\",\n",
    "    \"7701\": \"Leighcan family, warm, extremely stony.\",\n",
    "    \"7702\": \"Granile - Catamount families complex, very stony.\",\n",
    "    \"7709\": \"Leighcan family, warm - Rock outcrop complex, extremely stony.\",\n",
    "    \"7710\": \"Leighcan family - Rock outcrop complex, extremely stony.\",\n",
    "    \"7745\": \"Como - Legault families complex, extremely stony.\",\n",
    "    \"7746\": \"Como family - Rock land - Legault family complex, extremely stony.\",\n",
    "    \"7755\": \"Leighcan - Catamount families complex, extremely stony.\",\n",
    "    \"7756\": \"Catamount family - Rock outcrop - Leighcan family complex, extremely stony.\",\n",
    "    \"7757\": \"Leighcan - Catamount families - Rock outcrop complex, extremely stony.\",\n",
    "    \"7790\": \"Cryorthents - Rock land complex, extremely stony.\",\n",
    "    \"8703\": \"Cryumbrepts - Rock outcrop - Cryaquepts complex.\",\n",
    "    \"8707\": \"Bross family - Rock land - Cryumbrepts complex, extremely stony.\",\n",
    "    \"8708\": \"Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony.\",\n",
    "    \"8771\": \"Leighcan - Moran families - Cryaquolls complex, extremely stony.\",\n",
    "    \"8772\": \"Moran family - Cryorthents - Leighcan family complex, extremely stony.\",\n",
    "    \"8776\": \"Moran family - Cryorthents - Rock land complex, extremely stony.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in soil_types.items():\n",
    "    fun = re.split(' - |, ', v.lower().replace(\".\", \"\"))\n",
    "    colocations = [i.replace(\" \", \"_\") for i in fun]\n",
    "    soil_types[k] = \" \".join(colocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2702': 'cathedral_family rock_outcrop_complex extremely_stony',\n",
       " '2703': 'vanet ratake_families_complex very_stony',\n",
       " '2704': 'haploborolis rock_outcrop_complex rubbly',\n",
       " '2705': 'ratake_family rock_outcrop_complex rubbly',\n",
       " '2706': 'vanet_family rock_outcrop_complex_complex rubbly',\n",
       " '2717': 'vanet wetmore_families rock_outcrop_complex stony',\n",
       " '3501': 'gothic_family',\n",
       " '3502': 'supervisor limber_families_complex',\n",
       " '4201': 'troutville_family very_stony',\n",
       " '4703': 'bullwark catamount_families rock_outcrop_complex rubbly',\n",
       " '4704': 'bullwark catamount_families rock_land_complex rubbly',\n",
       " '4744': 'legault_family rock_land_complex stony',\n",
       " '4758': 'catamount_family rock_land bullwark_family_complex rubbly',\n",
       " '5101': 'pachic_argiborolis aquolis_complex',\n",
       " '5151': 'not_in_survey',\n",
       " '6101': 'cryaquolis cryoborolis_complex',\n",
       " '6102': 'gateview_family cryaquolis_complex',\n",
       " '6731': 'rogert_family very_stony',\n",
       " '7101': 'typic_cryaquolis borohemists_complex',\n",
       " '7102': 'typic_cryaquepts typic_cryaquolls_complex',\n",
       " '7103': 'typic_cryaquolls leighcan_family till_substratum_complex',\n",
       " '7201': 'leighcan_family till_substratum extremely_bouldery',\n",
       " '7202': 'leighcan_family till_substratum typic_cryaquolls_complex',\n",
       " '7700': 'leighcan_family extremely_stony',\n",
       " '7701': 'leighcan_family warm extremely_stony',\n",
       " '7702': 'granile catamount_families_complex very_stony',\n",
       " '7709': 'leighcan_family warm rock_outcrop_complex extremely_stony',\n",
       " '7710': 'leighcan_family rock_outcrop_complex extremely_stony',\n",
       " '7745': 'como legault_families_complex extremely_stony',\n",
       " '7746': 'como_family rock_land legault_family_complex extremely_stony',\n",
       " '7755': 'leighcan catamount_families_complex extremely_stony',\n",
       " '7756': 'catamount_family rock_outcrop leighcan_family_complex extremely_stony',\n",
       " '7757': 'leighcan catamount_families rock_outcrop_complex extremely_stony',\n",
       " '7790': 'cryorthents rock_land_complex extremely_stony',\n",
       " '8703': 'cryumbrepts rock_outcrop cryaquepts_complex',\n",
       " '8707': 'bross_family rock_land cryumbrepts_complex extremely_stony',\n",
       " '8708': 'rock_outcrop cryumbrepts cryorthents_complex extremely_stony',\n",
       " '8771': 'leighcan moran_families cryaquolls_complex extremely_stony',\n",
       " '8772': 'moran_family cryorthents leighcan_family_complex extremely_stony',\n",
       " '8776': 'moran_family cryorthents rock_land_complex extremely_stony'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First digit:  climatic zone   \n",
    "first_digit = { \"1\": \"lower montane dry\",\n",
    "                \"2\": \"lower montane\",          \n",
    "                \"3\": \"montane dry\",            \n",
    "                \"4\": \"montane\",                \n",
    "                \"5\": \"montane dry and montane\",\n",
    "                \"6\": \"montane and subalpine\",\n",
    "                \"7\": \"subalpine\",  \n",
    "                \"8\": \"alpine\" \n",
    "              }  \n",
    "\n",
    "# Second digit:  geologic zones\n",
    "second_digit = {\"1\": \"alluvium\",\n",
    "                \"2\": \"glacial\",\n",
    "                \"3\": \"shale\",\n",
    "                \"4\": \"sandstone\",\n",
    "                \"5\": \"mixed sedimentary\",\n",
    "                \"6\": \"not_in_survey\", #\"unspecified in the USFS ELU Survey\"\n",
    "                \"7\": \"igneous and metamorphic\",\n",
    "                \"8\": \"volcanic\"\n",
    "               }\n",
    "\n",
    "# The third and fourth ELU digits are unique to the mapping unit and \n",
    "# have no special meaning to the climatic or geologic zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in first_digit.items():\n",
    "    first_digit[k] = v.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'lower_montane_dry',\n",
       " '2': 'lower_montane',\n",
       " '3': 'montane_dry',\n",
       " '4': 'montane',\n",
       " '5': 'montane_dry_and_montane',\n",
       " '6': 'montane_and_subalpine',\n",
       " '7': 'subalpine',\n",
       " '8': 'alpine'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in second_digit.items():\n",
    "    second_digit[k] = v.replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': 'alluvium',\n",
       " '2': 'glacial',\n",
       " '3': 'shale',\n",
       " '4': 'sandstone',\n",
       " '5': 'mixed_sedimentary',\n",
       " '6': 'not_in_survey',\n",
       " '7': 'igneous_and_metamorphic',\n",
       " '8': 'volcanic'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_types_extend = defaultdict(str, soil_types)\n",
    "\n",
    "for k in soil_types_extend.keys():\n",
    "    climatic = \"climatic_zone_\" + first_digit.get(k[0])\n",
    "    geologic = \"geologic_zone_\" + second_digit.get(k[1])\n",
    "    soil_types_extend[k] += \" \" + climatic + \" \" + geologic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(str,\n",
       "            {'2702': 'cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2703': 'vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2704': 'haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2705': 'ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2706': 'vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '2717': 'vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '3501': 'gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary',\n",
       "             '3502': 'supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary',\n",
       "             '4201': 'troutville_family very_stony climatic_zone_montane geologic_zone_glacial',\n",
       "             '4703': 'bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4704': 'bullwark catamount_families rock_land_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4744': 'legault_family rock_land_complex stony climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '4758': 'catamount_family rock_land bullwark_family_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic',\n",
       "             '5101': 'pachic_argiborolis aquolis_complex climatic_zone_montane_dry_and_montane geologic_zone_alluvium',\n",
       "             '5151': 'not_in_survey climatic_zone_montane_dry_and_montane geologic_zone_alluvium',\n",
       "             '6101': 'cryaquolis cryoborolis_complex climatic_zone_montane_and_subalpine geologic_zone_alluvium',\n",
       "             '6102': 'gateview_family cryaquolis_complex climatic_zone_montane_and_subalpine geologic_zone_alluvium',\n",
       "             '6731': 'rogert_family very_stony climatic_zone_montane_and_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7101': 'typic_cryaquolis borohemists_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7102': 'typic_cryaquepts typic_cryaquolls_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7103': 'typic_cryaquolls leighcan_family till_substratum_complex climatic_zone_subalpine geologic_zone_alluvium',\n",
       "             '7201': 'leighcan_family till_substratum extremely_bouldery climatic_zone_subalpine geologic_zone_glacial',\n",
       "             '7202': 'leighcan_family till_substratum typic_cryaquolls_complex climatic_zone_subalpine geologic_zone_glacial',\n",
       "             '7700': 'leighcan_family extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7701': 'leighcan_family warm extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7702': 'granile catamount_families_complex very_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7709': 'leighcan_family warm rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7710': 'leighcan_family rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7745': 'como legault_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7746': 'como_family rock_land legault_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7755': 'leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7756': 'catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7757': 'leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '7790': 'cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8703': 'cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8707': 'bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8708': 'rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8771': 'leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8772': 'moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic',\n",
       "             '8776': 'moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_types_extend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The wilderness areas are\n",
    "wilderness_areas =   {'Wilderness_Area1': \"Rawah Wilderness Area\", \n",
    "                      'Wilderness_Area2': \"Neota Wilderness Area\",\n",
    "                      'Wilderness_Area3': \"Comanche Peak Wilderness Area\",\n",
    "                      'Wilderness_Area4': \"Cache la Poudre Wilderness Area\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in wilderness_areas.items():\n",
    "    wilderness_areas[k] = v.lower().replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Wilderness_Area1': 'rawah_wilderness_area',\n",
       " 'Wilderness_Area2': 'neota_wilderness_area',\n",
       " 'Wilderness_Area3': 'comanche_peak_wilderness_area',\n",
       " 'Wilderness_Area4': 'cache_la_poudre_wilderness_area'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wilderness_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soil_cols = list(soil_types_extend.values())\n",
    "wilderness_cols = list(wilderness_areas.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
    "#        'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
    "#        'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
    "#        'Horizontal_Distance_To_Fire_Points'] \n",
    "# cols += wilderness_cols + soil_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawah_wilderness_area</th>\n",
       "      <th>neota_wilderness_area</th>\n",
       "      <th>comanche_peak_wilderness_area</th>\n",
       "      <th>cache_la_poudre_wilderness_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rawah_wilderness_area  neota_wilderness_area  \\\n",
       "0                    0.0                    0.0   \n",
       "1                    0.0                    0.0   \n",
       "2                    0.0                    0.0   \n",
       "3                    0.0                    0.0   \n",
       "4                    0.0                    0.0   \n",
       "\n",
       "   comanche_peak_wilderness_area  cache_la_poudre_wilderness_area  \n",
       "0                            1.0                              0.0  \n",
       "1                            1.0                              0.0  \n",
       "2                            1.0                              0.0  \n",
       "3                            1.0                              0.0  \n",
       "4                            1.0                              0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_test = pd.DataFrame(X_res[:, 10:14], columns=wilderness_cols).head()\n",
    "wild_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area\n",
       "1    comanche_peak_wilderness_area\n",
       "2    comanche_peak_wilderness_area\n",
       "3    comanche_peak_wilderness_area\n",
       "4    comanche_peak_wilderness_area\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(wild_test.idxmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary</th>\n",
       "      <th>supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary</th>\n",
       "      <th>troutville_family very_stony climatic_zone_montane geologic_zone_glacial</th>\n",
       "      <th>bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>...</th>\n",
       "      <th>leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "      <th>moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cathedral_family rock_outcrop_complex extremely_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                         \n",
       "1                                                0.0                                                                         \n",
       "2                                                0.0                                                                         \n",
       "3                                                0.0                                                                         \n",
       "4                                                0.0                                                                         \n",
       "\n",
       "   vanet ratake_families_complex very_stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                            \n",
       "1                                                0.0                                                            \n",
       "2                                                0.0                                                            \n",
       "3                                                0.0                                                            \n",
       "4                                                0.0                                                            \n",
       "\n",
       "   haploborolis rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                            \n",
       "1                                                0.0                                                            \n",
       "2                                                0.0                                                            \n",
       "3                                                0.0                                                            \n",
       "4                                                0.0                                                            \n",
       "\n",
       "   ratake_family rock_outcrop_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                             \n",
       "1                                                0.0                                                             \n",
       "2                                                0.0                                                             \n",
       "3                                                0.0                                                             \n",
       "4                                                0.0                                                             \n",
       "\n",
       "   vanet_family rock_outcrop_complex_complex rubbly climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                    \n",
       "1                                                0.0                                                                    \n",
       "2                                                0.0                                                                    \n",
       "3                                                0.0                                                                    \n",
       "4                                                0.0                                                                    \n",
       "\n",
       "   vanet wetmore_families rock_outcrop_complex stony climatic_zone_lower_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                     \n",
       "1                                                0.0                                                                     \n",
       "2                                                0.0                                                                     \n",
       "3                                                0.0                                                                     \n",
       "4                                                0.0                                                                     \n",
       "\n",
       "   gothic_family climatic_zone_montane_dry geologic_zone_mixed_sedimentary  \\\n",
       "0                                                0.0                         \n",
       "1                                                0.0                         \n",
       "2                                                0.0                         \n",
       "3                                                0.0                         \n",
       "4                                                0.0                         \n",
       "\n",
       "   supervisor limber_families_complex climatic_zone_montane_dry geologic_zone_mixed_sedimentary  \\\n",
       "0                                                0.0                                              \n",
       "1                                                0.0                                              \n",
       "2                                                0.0                                              \n",
       "3                                                0.0                                              \n",
       "4                                                0.0                                              \n",
       "\n",
       "   troutville_family very_stony climatic_zone_montane geologic_zone_glacial  \\\n",
       "0                                                0.0                          \n",
       "1                                                0.0                          \n",
       "2                                                0.0                          \n",
       "3                                                0.0                          \n",
       "4                                                0.0                          \n",
       "\n",
       "   bullwark catamount_families rock_outcrop_complex rubbly climatic_zone_montane geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                     \n",
       "1                                                0.0                                                                     \n",
       "2                                                0.0                                                                     \n",
       "3                                                0.0                                                                     \n",
       "4                                                0.0                                                                     \n",
       "\n",
       "                                                           ...                                                            \\\n",
       "0                                                          ...                                                             \n",
       "1                                                          ...                                                             \n",
       "2                                                          ...                                                             \n",
       "3                                                          ...                                                             \n",
       "4                                                          ...                                                             \n",
       "\n",
       "   leighcan catamount_families_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                   \n",
       "1                                                0.0                                                                   \n",
       "2                                                1.0                                                                   \n",
       "3                                                0.0                                                                   \n",
       "4                                                0.0                                                                   \n",
       "\n",
       "   catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                1.0                                                                                     \n",
       "1                                                0.0                                                                                     \n",
       "2                                                0.0                                                                                     \n",
       "3                                                1.0                                                                                     \n",
       "4                                                0.0                                                                                     \n",
       "\n",
       "   leighcan catamount_families rock_outcrop_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                                \n",
       "1                                                0.0                                                                                \n",
       "2                                                0.0                                                                                \n",
       "3                                                0.0                                                                                \n",
       "4                                                0.0                                                                                \n",
       "\n",
       "   cryorthents rock_land_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                             \n",
       "1                                                0.0                                                             \n",
       "2                                                0.0                                                             \n",
       "3                                                0.0                                                             \n",
       "4                                                0.0                                                             \n",
       "\n",
       "   cryumbrepts rock_outcrop cryaquepts_complex climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                        \n",
       "1                                                0.0                                                        \n",
       "2                                                0.0                                                        \n",
       "3                                                0.0                                                        \n",
       "4                                                0.0                                                        \n",
       "\n",
       "   bross_family rock_land cryumbrepts_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                       \n",
       "1                                                0.0                                                                       \n",
       "2                                                0.0                                                                       \n",
       "3                                                0.0                                                                       \n",
       "4                                                0.0                                                                       \n",
       "\n",
       "   rock_outcrop cryumbrepts cryorthents_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                         \n",
       "1                                                0.0                                                                         \n",
       "2                                                0.0                                                                         \n",
       "3                                                0.0                                                                         \n",
       "4                                                0.0                                                                         \n",
       "\n",
       "   leighcan moran_families cryaquolls_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                       \n",
       "1                                                0.0                                                                       \n",
       "2                                                0.0                                                                       \n",
       "3                                                0.0                                                                       \n",
       "4                                                0.0                                                                       \n",
       "\n",
       "   moran_family cryorthents leighcan_family_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \\\n",
       "0                                                0.0                                                                             \n",
       "1                                                0.0                                                                             \n",
       "2                                                0.0                                                                             \n",
       "3                                                0.0                                                                             \n",
       "4                                                0.0                                                                             \n",
       "\n",
       "   moran_family cryorthents rock_land_complex extremely_stony climatic_zone_alpine geologic_zone_igneous_and_metamorphic  \n",
       "0                                                0.0                                                                      \n",
       "1                                                1.0                                                                      \n",
       "2                                                0.0                                                                      \n",
       "3                                                0.0                                                                      \n",
       "4                                                0.0                                                                      \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_test = pd.DataFrame(X_res[:, 14:], columns=soil_cols).head()\n",
    "soil_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area. catamount_famil...\n",
       "1    comanche_peak_wilderness_area. moran_family cr...\n",
       "2    comanche_peak_wilderness_area. leighcan catamo...\n",
       "3    comanche_peak_wilderness_area. catamount_famil...\n",
       "4    comanche_peak_wilderness_area. leighcan_family...\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_test.idxmax(axis=1) + \". \" + soil_test.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    comanche_peak_wilderness_area catamount_family...\n",
       "1    comanche_peak_wilderness_area moran_family cry...\n",
       "2    comanche_peak_wilderness_area leighcan catamou...\n",
       "3    comanche_peak_wilderness_area catamount_family...\n",
       "4    comanche_peak_wilderness_area leighcan_family ...\n",
       "5    comanche_peak_wilderness_area catamount_family...\n",
       "6    neota_wilderness_area catamount_family rock_ou...\n",
       "7    rawah_wilderness_area como legault_families_co...\n",
       "8    rawah_wilderness_area leighcan_family till_sub...\n",
       "9    comanche_peak_wilderness_area catamount_family...\n",
       "dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_df = pd.DataFrame(X_res[:, 10:14], columns=wilderness_cols)\n",
    "\n",
    "soil_df = pd.DataFrame(X_res[:, 14:], columns=soil_cols)\n",
    "\n",
    "X_wild_soil = wild_df.idxmax(axis=1) + \" \" + soil_df.idxmax(axis=1)\n",
    "X_wild_soil.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comanche_peak_wilderness_area catamount_family rock_outcrop leighcan_family_complex extremely_stony climatic_zone_subalpine geologic_zone_igneous_and_metamorphic'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_wild_soil[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>soil_type</th>\n",
       "      <th>wild_soil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>comanche_peak_wilderness_area catamount_family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>comanche_peak_wilderness_area moran_family cry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>comanche_peak_wilderness_area leighcan catamou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>comanche_peak_wilderness_area catamount_family...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>comanche_peak_wilderness_area leighcan_family ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   soil_type                                          wild_soil\n",
       "0          1  comanche_peak_wilderness_area catamount_family...\n",
       "1          1  comanche_peak_wilderness_area moran_family cry...\n",
       "2          1  comanche_peak_wilderness_area leighcan catamou...\n",
       "3          1  comanche_peak_wilderness_area catamount_family...\n",
       "4          1  comanche_peak_wilderness_area leighcan_family ..."
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_res = pd.Series(y_res) #targets from random undersampler \n",
    "X_wild_soil_w_targets = pd.DataFrame({\"wild_soil\": X_wild_soil, \"soil_type\":targets_res})\n",
    "X_wild_soil_w_targets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Catamount_family,': 1,\n",
       "         'Comanche_Peak_Wilderness_Area.': 1,\n",
       "         'Leighcan_family_complex,': 1,\n",
       "         'Rock_outcrop,': 1,\n",
       "         'climatic_zone_subalpine,': 1,\n",
       "         'extremely_stony,': 1,\n",
       "         'geologic_zone_igneous_and_metamorphic': 1})"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Counter()\n",
    "\n",
    "a = query[0]\n",
    "b = list(a.split(\" \"))\n",
    "test.update(b)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('climatic_zone_subalpine', 2352),\n",
       " ('geologic_zone_igneous_and_metamorphic', 1831),\n",
       " ('extremely_stony', 1721),\n",
       " ('rawah_wilderness_area', 1357),\n",
       " ('comanche_peak_wilderness_area', 1140),\n",
       " ('leighcan_family', 985),\n",
       " ('geologic_zone_glacial', 824),\n",
       " ('till_substratum', 821),\n",
       " ('como', 535),\n",
       " ('legault_families_complex', 535)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 1\n",
    "query = X_wild_soil_w_targets.query(\"soil_type == {}\".format(i))[\"wild_soil\"]\n",
    "test = Counter()\n",
    "for row in query:\n",
    "    words = list(row.split(\" \"))\n",
    "    test.update(words)\n",
    "test.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climatic_zone_subalpine',\n",
       " 'geologic_zone_igneous_and_metamorphic',\n",
       " 'extremely_stony',\n",
       " 'rawah_wilderness_area',\n",
       " 'comanche_peak_wilderness_area',\n",
       " 'leighcan_family',\n",
       " 'geologic_zone_glacial',\n",
       " 'till_substratum',\n",
       " 'como',\n",
       " 'legault_families_complex']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i[0] for i in test.most_common(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Type 1:\n",
      "Unique words in Soil Type 1 vocabulary: 57\n",
      "Top 10 words in Soil Type 1: ['climatic_zone_subalpine', 'geologic_zone_igneous_and_metamorphic', 'extremely_stony', 'rawah_wilderness_area', 'comanche_peak_wilderness_area', 'leighcan_family', 'geologic_zone_glacial', 'till_substratum', 'como', 'legault_families_complex'] \n",
      "\n",
      "Soil Type 2:\n",
      "Unique words in Soil Type 2 vocabulary: 61\n",
      "Top 10 words in Soil Type 2: ['geologic_zone_igneous_and_metamorphic', 'climatic_zone_subalpine', 'extremely_stony', 'rawah_wilderness_area', 'comanche_peak_wilderness_area', 'como', 'legault_families_complex', 'climatic_zone_montane', 'catamount_families', 'rock_outcrop_complex'] \n",
      "\n",
      "Soil Type 3:\n",
      "Unique words in Soil Type 3 vocabulary: 36\n",
      "Top 10 words in Soil Type 3: ['geologic_zone_igneous_and_metamorphic', 'rock_outcrop_complex', 'rubbly', 'climatic_zone_lower_montane', 'cache_la_poudre_wilderness_area', 'comanche_peak_wilderness_area', 'climatic_zone_montane', 'bullwark', 'catamount_families', 'vanet'] \n",
      "\n",
      "Soil Type 4:\n",
      "Unique words in Soil Type 4 vocabulary: 29\n",
      "Top 10 words in Soil Type 4: ['cache_la_poudre_wilderness_area', 'geologic_zone_igneous_and_metamorphic', 'rock_outcrop_complex', 'climatic_zone_lower_montane', 'rubbly', 'haploborolis', 'geologic_zone_alluvium', 'climatic_zone_montane_and_subalpine', 'gateview_family', 'cryaquolis_complex'] \n",
      "\n",
      "Soil Type 5:\n",
      "Unique words in Soil Type 5 vocabulary: 43\n",
      "Top 10 words in Soil Type 5: ['geologic_zone_igneous_and_metamorphic', 'comanche_peak_wilderness_area', 'climatic_zone_subalpine', 'extremely_stony', 'rawah_wilderness_area', 'rock_land', 'rubbly', 'climatic_zone_montane', 'como_family', 'legault_family_complex'] \n",
      "\n",
      "Soil Type 6:\n",
      "Unique words in Soil Type 6 vocabulary: 45\n",
      "Top 10 words in Soil Type 6: ['geologic_zone_igneous_and_metamorphic', 'rock_outcrop_complex', 'rubbly', 'climatic_zone_montane', 'catamount_families', 'cache_la_poudre_wilderness_area', 'bullwark', 'comanche_peak_wilderness_area', 'climatic_zone_lower_montane', 'vanet'] \n",
      "\n",
      "Soil Type 7:\n",
      "Unique words in Soil Type 7 vocabulary: 43\n",
      "Top 10 words in Soil Type 7: ['geologic_zone_igneous_and_metamorphic', 'extremely_stony', 'climatic_zone_alpine', 'comanche_peak_wilderness_area', 'cryorthents', 'moran_family', 'leighcan', 'leighcan_family_complex', 'moran_families', 'cryaquolls_complex'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "soil_CountVect = dict()\n",
    "for i in range(1, 8): \n",
    "    print(\"Soil Type {}:\".format(i))\n",
    "    query = X_wild_soil_w_targets.query(\"soil_type == {}\".format(i))[\"wild_soil\"]\n",
    "    temp_counter = Counter()\n",
    "    for row in query:\n",
    "        words = list(row.split(\" \"))\n",
    "        temp_counter.update(words)\n",
    "    top_10 = [i[0] for i in temp_counter.most_common(10)]\n",
    "    print(\"Unique words in Soil Type {} vocabulary: {}\".format(i, len(temp_counter)))\n",
    "    print(\"Top 10 words in Soil Type {}: {} \\n\".format(i, top_10))        \n",
    "    soil_CountVect[\"Soil Type {}:\".format(i)] = \", \".join(top_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 75), set())"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer for entire dataset\n",
    "min_df = 1 # default making a point to keep all features if min_df=2 then only token removed is 'not_in_survey'\n",
    "max_df = 0.95 # unless they appear in all docs \n",
    "max_features = 100\n",
    "vectorizer = CountVectorizer(max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "\n",
    "vectorized = vectorizer.fit_transform(X_wild_soil)\n",
    "vectorized.shape, vectorizer.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NMF(init=\"nndsvd\",\n",
    "            n_components=7,\n",
    "            max_iter=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.fit_transform(vectorized)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 7), (7, 75))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape, H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = [\"\"] * len(vectorizer.vocabulary_)\n",
    "for term in vectorizer.vocabulary_.keys():\n",
    "    terms[vectorizer.vocabulary_[term]] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Topic 1: catamount_families, bullwark, climatic_zone_montane, rubbly, rock_outcrop_complex, geologic_zone_igneous_and_metamorphic, cache_la_poudre_wilderness_area, rock_land_complex, comanche_peak_wilderness_area, legault_family \n",
      "\n",
      "Soil Topic 2: rawah_wilderness_area, geologic_zone_igneous_and_metamorphic, extremely_stony, climatic_zone_subalpine, legault_families_complex, como, legault_family_complex, como_family, rock_land, legault_family \n",
      "\n",
      "Soil Topic 3: climatic_zone_lower_montane, cache_la_poudre_wilderness_area, geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, vanet, haploborolis, stony, wetmore_families, ratake_family \n",
      "\n",
      "Soil Topic 4: leighcan_family, geologic_zone_glacial, till_substratum, climatic_zone_subalpine, typic_cryaquolls_complex, rawah_wilderness_area, extremely_bouldery, comanche_peak_wilderness_area, neota_wilderness_area, geologic_zone_alluvium \n",
      "\n",
      "Soil Topic 5: extremely_stony, comanche_peak_wilderness_area, climatic_zone_subalpine, geologic_zone_igneous_and_metamorphic, leighcan, rock_outcrop, leighcan_family_complex, catamount_family, rock_outcrop_complex, catamount_families \n",
      "\n",
      "Soil Topic 6: climatic_zone_alpine, extremely_stony, geologic_zone_igneous_and_metamorphic, cryorthents, moran_family, leighcan_family_complex, comanche_peak_wilderness_area, rock_land_complex, cryaquolls_complex, moran_families \n",
      "\n",
      "Soil Topic 7: comanche_peak_wilderness_area, rubbly, geologic_zone_igneous_and_metamorphic, rock_land, climatic_zone_montane, catamount_family, bullwark_family_complex, ratake_family, climatic_zone_lower_montane, rock_land_complex \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_index in range(H.shape[0]):\n",
    "    top_indicies = np.argsort(H[topic_index, :])[::-1][0:10]\n",
    "    term_ranking = [terms[i] for i in top_indicies]\n",
    "    print(\"Soil Topic {}: {} \\n\".format(topic_index+1, \", \".join(term_ranking)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Type 1: climatic_zone_subalpine, geologic_zone_igneous_and_metamorphic, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, leighcan_family, geologic_zone_glacial, till_substratum, como, legault_families_complex\n",
      "\n",
      "Soil Type 2: geologic_zone_igneous_and_metamorphic, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, como, legault_families_complex, climatic_zone_montane, catamount_families, rock_outcrop_complex\n",
      "\n",
      "Soil Type 3: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_lower_montane, cache_la_poudre_wilderness_area, comanche_peak_wilderness_area, climatic_zone_montane, bullwark, catamount_families, vanet\n",
      "\n",
      "Soil Type 4: cache_la_poudre_wilderness_area, geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, climatic_zone_lower_montane, rubbly, haploborolis, geologic_zone_alluvium, climatic_zone_montane_and_subalpine, gateview_family, cryaquolis_complex\n",
      "\n",
      "Soil Type 5: geologic_zone_igneous_and_metamorphic, comanche_peak_wilderness_area, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, rock_land, rubbly, climatic_zone_montane, como_family, legault_family_complex\n",
      "\n",
      "Soil Type 6: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_montane, catamount_families, cache_la_poudre_wilderness_area, bullwark, comanche_peak_wilderness_area, climatic_zone_lower_montane, vanet\n",
      "\n",
      "Soil Type 7: geologic_zone_igneous_and_metamorphic, extremely_stony, climatic_zone_alpine, comanche_peak_wilderness_area, cryorthents, moran_family, leighcan, leighcan_family_complex, moran_families, cryaquolls_complex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in soil_CountVect.items():\n",
    "    v_str = v\n",
    "    print(k, v_str)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 75), set())"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tfidf on entire dataset (balanced)\n",
    "min_df = 1 # default making a point to keep all features \n",
    "max_df = 0.95 # unless they appear in all docs \n",
    "max_features = 100 # not a problem here... soil + wilderness has a max of 70 feats\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(max_features=max_features, max_df=max_df, min_df=min_df)\n",
    "\n",
    "tfidf_vecD = tfidf_vec.fit_transform(X_wild_soil)\n",
    "tfidf_vecD.shape, tfidf_vec.stop_words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "terms = [\"\"] * len(tfidf_vec.vocabulary_)\n",
    "for term in tfidf_vec.vocabulary_.keys():\n",
    "    terms[tfidf_vec.vocabulary_[term]] = term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19229, 7), (7, 75))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tfidf = NMF(init=\"nndsvd\",\n",
    "                n_components=7,\n",
    "                max_iter=200)\n",
    "\n",
    "W_tfidf = model_tfidf.fit_transform(tfidf_vecD)\n",
    "H_tfidf = model_tfidf.components_\n",
    "\n",
    "W_tfidf.shape, H_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Type 1: bullwark, climatic_zone_montane, catamount_families, rubbly, rock_outcrop_complex, cache_la_poudre_wilderness_area, geologic_zone_igneous_and_metamorphic, comanche_peak_wilderness_area, rock_land_complex, bullwark_family_complex\n",
      "\n",
      "Soil Type 2: legault_families_complex, como, rawah_wilderness_area, climatic_zone_subalpine, extremely_stony, geologic_zone_igneous_and_metamorphic, cathedral_family, legault_family, leighcan_family, stony\n",
      "\n",
      "Soil Type 3: climatic_zone_lower_montane, rock_outcrop_complex, cache_la_poudre_wilderness_area, haploborolis, rubbly, geologic_zone_igneous_and_metamorphic, ratake_family, vanet, wetmore_families, stony\n",
      "\n",
      "Soil Type 4: cryorthents, moran_family, climatic_zone_alpine, leighcan_family_complex, extremely_stony, rock_land_complex, comanche_peak_wilderness_area, geologic_zone_igneous_and_metamorphic, rock_outcrop, neota_wilderness_area\n",
      "\n",
      "Soil Type 5: leighcan_family, till_substratum, geologic_zone_glacial, typic_cryaquolls_complex, climatic_zone_subalpine, extremely_bouldery, comanche_peak_wilderness_area, rawah_wilderness_area, neota_wilderness_area, geologic_zone_alluvium\n",
      "\n",
      "Soil Type 6: rock_land, como_family, legault_family_complex, catamount_family, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, bullwark_family_complex, geologic_zone_igneous_and_metamorphic, rock_outcrop\n",
      "\n",
      "Soil Type 7: leighcan, extremely_stony, comanche_peak_wilderness_area, moran_families, cryaquolls_complex, climatic_zone_subalpine, climatic_zone_alpine, geologic_zone_igneous_and_metamorphic, catamount_families_complex, catamount_families\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for topic_index in range(H_tfidf.shape[0]):\n",
    "    top_indices = np.argsort(H_tfidf[topic_index,:])[::-1][0:10]\n",
    "    term_ranking = [terms[i] for i in top_indices]\n",
    "    print(\"Soil Type {}: {}\\n\".format(topic_index+1, \", \".join(term_ranking)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Type 1: climatic_zone_subalpine, geologic_zone_igneous_and_metamorphic, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, leighcan_family, geologic_zone_glacial, till_substratum, como, legault_families_complex\n",
      "\n",
      "Soil Type 2: geologic_zone_igneous_and_metamorphic, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, como, legault_families_complex, climatic_zone_montane, catamount_families, rock_outcrop_complex\n",
      "\n",
      "Soil Type 3: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_lower_montane, cache_la_poudre_wilderness_area, comanche_peak_wilderness_area, climatic_zone_montane, bullwark, catamount_families, vanet\n",
      "\n",
      "Soil Type 4: cache_la_poudre_wilderness_area, geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, climatic_zone_lower_montane, rubbly, haploborolis, geologic_zone_alluvium, climatic_zone_montane_and_subalpine, gateview_family, cryaquolis_complex\n",
      "\n",
      "Soil Type 5: geologic_zone_igneous_and_metamorphic, comanche_peak_wilderness_area, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, rock_land, rubbly, climatic_zone_montane, como_family, legault_family_complex\n",
      "\n",
      "Soil Type 6: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_montane, catamount_families, cache_la_poudre_wilderness_area, bullwark, comanche_peak_wilderness_area, climatic_zone_lower_montane, vanet\n",
      "\n",
      "Soil Type 7: geologic_zone_igneous_and_metamorphic, extremely_stony, climatic_zone_alpine, comanche_peak_wilderness_area, cryorthents, moran_family, leighcan, leighcan_family_complex, moran_families, cryaquolls_complex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in soil_CountVect.items():\n",
    "    v_str = v\n",
    "    print(k, v_str)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition.online_lda import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=7,\n",
    "                                max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_norm = lda.components_ / lda.components_.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.28146925e-05,   3.28232421e-05,   3.28147258e-05,\n",
       "         3.28147007e-05,   3.28146729e-05,   3.28148334e-05,\n",
       "         3.28154929e-05,   3.28158210e-05,   3.28184314e-05,\n",
       "         3.28146866e-05,   3.28239795e-05,   3.28146603e-05,\n",
       "         1.19141712e-02,   3.28172115e-05,   3.28176594e-05,\n",
       "         3.28146947e-05,   1.56319088e-01,   3.28320262e-05,\n",
       "         1.54291548e-01,   3.29442356e-05,   3.28228843e-05,\n",
       "         3.28183963e-05,   3.28146586e-05,   3.28226802e-05,\n",
       "         3.28184010e-05,   3.28146432e-05,   3.28177959e-05,\n",
       "         3.28217504e-05,   3.28147372e-05,   3.28145898e-05,\n",
       "         1.55615405e-01,   3.28146475e-05,   3.28174173e-05,\n",
       "         3.28145860e-05,   8.18844690e-02,   3.28176989e-05,\n",
       "         3.28201859e-05,   3.28145915e-05,   3.28146383e-05,\n",
       "         1.54291548e-01,   2.45012776e-02,   3.29445064e-05,\n",
       "         3.28199412e-05,   3.28276167e-05,   3.28183663e-05,\n",
       "         3.28182070e-05,   3.28227094e-05,   3.28146391e-05,\n",
       "         3.28164815e-05,   3.28151359e-05,   3.28147041e-05,\n",
       "         3.28146464e-05,   3.28146562e-05,   2.24173354e-01,\n",
       "         3.28877702e-05,   1.69176325e-02,   3.28198443e-05,\n",
       "         3.28154813e-05,   3.28146954e-05,   3.28409887e-05,\n",
       "         3.28147818e-05,   1.79580399e-02,   3.28182426e-05,\n",
       "         3.28145927e-05,   3.28147581e-05,   3.28147129e-05,\n",
       "         3.28145994e-05,   3.28232740e-05,   3.28147709e-05,\n",
       "         3.28146200e-05,   3.28146785e-05,   3.28146685e-05,\n",
       "         3.28233396e-05,   3.28146377e-05,   3.28147584e-05])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_norm[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=7, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=42, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tfidf_vecD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 56, 44, 17, 16, 30, 34, 66, 32, 69])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].argsort()[:-11:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: catamount_family, rock_outcrop, leighcan_family_complex, comanche_peak_wilderness_area, climatic_zone_subalpine, extremely_stony, geologic_zone_igneous_and_metamorphic, typic_cryaquepts, geologic_zone_alluvium, typic_cryaquolls_complex\n",
      "\n",
      "Topic 2: climatic_zone_montane, bullwark, rubbly, cache_la_poudre_wilderness_area, rock_land, geologic_zone_igneous_and_metamorphic, como_family, legault_family_complex, climatic_zone_montane_and_subalpine, bullwark_family_complex\n",
      "\n",
      "Topic 3: cryorthents, moran_family, climatic_zone_alpine, rock_land_complex, extremely_stony, leighcan_family_complex, comanche_peak_wilderness_area, geologic_zone_igneous_and_metamorphic, neota_wilderness_area, catamount_families_complex\n",
      "\n",
      "Topic 4: catamount_families, rock_outcrop_complex, leighcan, comanche_peak_wilderness_area, cryaquolls_complex, moran_families, geologic_zone_igneous_and_metamorphic, extremely_stony, climatic_zone_alpine, climatic_zone_subalpine\n",
      "\n",
      "Topic 5: rawah_wilderness_area, climatic_zone_subalpine, extremely_stony, legault_families_complex, como, geologic_zone_igneous_and_metamorphic, legault_family, stony, rock_land_complex, climatic_zone_montane\n",
      "\n",
      "Topic 6: climatic_zone_lower_montane, rock_outcrop_complex, cache_la_poudre_wilderness_area, rubbly, geologic_zone_igneous_and_metamorphic, haploborolis, vanet, ratake_family, very_stony, wetmore_families\n",
      "\n",
      "Topic 7: leighcan_family, geologic_zone_glacial, till_substratum, typic_cryaquolls_complex, climatic_zone_subalpine, comanche_peak_wilderness_area, rawah_wilderness_area, extremely_bouldery, neota_wilderness_area, typic_cryaquolls\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tfidf_vec.get_feature_names()\n",
    "lda_topics = dict()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    lda_topics[\"Topic {}:\".format(topic_idx+1)] = [i for i in topic.argsort()[:-11:-1]] # keep track of indicies \n",
    "    print(\"Topic {}:\".format(topic_idx+1), \", \".join([tf_feature_names[i] for i in topic.argsort()[:-11:-1]]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil Type 1: climatic_zone_subalpine, geologic_zone_igneous_and_metamorphic, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, leighcan_family, geologic_zone_glacial, till_substratum, como, legault_families_complex\n",
      "\n",
      "Soil Type 2: geologic_zone_igneous_and_metamorphic, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, comanche_peak_wilderness_area, como, legault_families_complex, climatic_zone_montane, catamount_families, rock_outcrop_complex\n",
      "\n",
      "Soil Type 3: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_lower_montane, cache_la_poudre_wilderness_area, comanche_peak_wilderness_area, climatic_zone_montane, bullwark, catamount_families, vanet\n",
      "\n",
      "Soil Type 4: cache_la_poudre_wilderness_area, geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, climatic_zone_lower_montane, rubbly, haploborolis, geologic_zone_alluvium, climatic_zone_montane_and_subalpine, gateview_family, cryaquolis_complex\n",
      "\n",
      "Soil Type 5: geologic_zone_igneous_and_metamorphic, comanche_peak_wilderness_area, climatic_zone_subalpine, extremely_stony, rawah_wilderness_area, rock_land, rubbly, climatic_zone_montane, como_family, legault_family_complex\n",
      "\n",
      "Soil Type 6: geologic_zone_igneous_and_metamorphic, rock_outcrop_complex, rubbly, climatic_zone_montane, catamount_families, cache_la_poudre_wilderness_area, bullwark, comanche_peak_wilderness_area, climatic_zone_lower_montane, vanet\n",
      "\n",
      "Soil Type 7: geologic_zone_igneous_and_metamorphic, extremely_stony, climatic_zone_alpine, comanche_peak_wilderness_area, cryorthents, moran_family, leighcan, leighcan_family_complex, moran_families, cryaquolls_complex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in soil_CountVect.items():\n",
    "    print(k, v)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topic 1:': [8, 56, 44, 17, 16, 30, 34, 66, 32, 69],\n",
       " 'Topic 2:': [12, 3, 60, 5, 54, 34, 19, 41, 13, 4],\n",
       " 'Topic 3:': [25, 47, 10, 55, 30, 44, 17, 34, 48, 7],\n",
       " 'Topic 4:': [6, 57, 42, 17, 23, 46, 34, 30, 10, 16],\n",
       " 'Topic 5:': [53, 16, 30, 39, 18, 34, 40, 61, 55, 12],\n",
       " 'Topic 6:': [11, 57, 5, 60, 34, 38, 70, 52, 72, 74],\n",
       " 'Topic 7:': [43, 33, 63, 69, 16, 17, 53, 29, 48, 68]}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['climatic_zone_subalpine',\n",
       " 'geologic_zone_igneous_and_metamorphic',\n",
       " 'extremely_stony',\n",
       " 'rawah_wilderness_area',\n",
       " 'comanche_peak_wilderness_area',\n",
       " 'leighcan_family',\n",
       " 'geologic_zone_glacial',\n",
       " 'till_substratum',\n",
       " 'como',\n",
       " 'legault_families_complex']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soil_CountVect[\"Soil Type 1:\"].split(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 53\n",
      "34 16\n",
      "30 30\n",
      "53 39\n",
      "17 18\n",
      "43 34\n",
      "33 40\n",
      "63 61\n",
      "18 55\n",
      "39 12\n"
     ]
    }
   ],
   "source": [
    "test_true = list()\n",
    "for y_true, y_pred in zip(soil_CountVect[\"Soil Type 1:\"].split(\", \"), lda_topics[\"Topic 5:\"]):\n",
    "    test_true.append(tf_feature_names.index(y_true))\n",
    "    print(tf_feature_names.index(y_true), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 34, 30, 53, 17, 43, 33, 63, 18, 39]\n",
      "[[53  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 16  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 30  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 39  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 34  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 40  0  0  0]\n",
      " [ 0  0  0  0  0  0  0 61  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 55  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "31.084898755419623"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# True soil type 1\n",
    "print(test_true)\n",
    "# predicted \"soil type 1\" from lda --> \"topic 5\"\n",
    "test_pred = np.diagflat(lda_topics[\"Topic 5:\"])\n",
    "print(test_pred)\n",
    "\n",
    "log_loss(test_true, test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.084898755419623"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test_true, test_pred, labels=test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16,\n",
       " 34,\n",
       " 30,\n",
       " 53,\n",
       " 17,\n",
       " 43,\n",
       " 33,\n",
       " 63,\n",
       " 18,\n",
       " 39,\n",
       " 69,\n",
       " 42,\n",
       " 44,\n",
       " 29,\n",
       " 8,\n",
       " 10,\n",
       " 56,\n",
       " 6,\n",
       " 57,\n",
       " 48,\n",
       " 47,\n",
       " 25,\n",
       " 7,\n",
       " 55,\n",
       " 54,\n",
       " 46,\n",
       " 23,\n",
       " 12,\n",
       " 32,\n",
       " 19,\n",
       " 41,\n",
       " 66,\n",
       " 60,\n",
       " 40,\n",
       " 61,\n",
       " 4,\n",
       " 67,\n",
       " 1,\n",
       " 3,\n",
       " 13,\n",
       " 72,\n",
       " 27,\n",
       " 20,\n",
       " 73,\n",
       " 31,\n",
       " 22,\n",
       " 37,\n",
       " 68,\n",
       " 64,\n",
       " 21,\n",
       " 24,\n",
       " 65,\n",
       " 59,\n",
       " 52,\n",
       " 11,\n",
       " 2,\n",
       " 28]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true2 = list()\n",
    "for y_true in test.most_common():\n",
    "    test_true2.append(tf_feature_names.index(y_true[0]))\n",
    "test_true2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wtf_mate = np.array((list(lda_norm[4][test_true2]) * len(test_true2))).reshape(len(test_true2),len(test_true2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.9982320008088728"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test_true2, wtf_mate, labels=test_true2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.539953534362336"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(test_true2, np.diagflat(lda_norm[4][test_true2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# fitting our training data with scikit learn's RandomForestClassifier\n",
    "# Choosing this algorithm over others due to our feature space. \n",
    "# 44 of our features are binary, whether or not the tree is in one of 4 wilderness areas\n",
    "# and whether or not the tree is found in one of 40 soil types\n",
    "# In the Forest_Cover_Type.ipynb, the goal was to correctly classify only one tree type (7)\n",
    "# Through data exploration, and the logit function it was found that nearly all of the \n",
    "# features were statistically significant. \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0. ,  0.1,  0. ,  0. ,  0.9,  0. ,  0. ],\n",
       "       [ 0. ,  0.1,  0. ,  0. ,  0.9,  0. ,  0. ],\n",
       "       [ 0.1,  0.9,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. ,  0. ,  0. ,  0. ,  0. ,  0. ],\n",
       "       [ 0. ,  0. ,  0. ,  0. ,  1. ,  0. ,  0. ]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = clf.predict_proba(X)[:5]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, 2, 5], dtype=int32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.063216309394701575"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y[:5], a, labels=np.arange(1,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9441155209803449"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using only 10 trees, we are able to predict with a very high accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=95787995, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srted_index():\n",
    "    idx_lst = []\n",
    "    for i, feat in enumerate(clf.feature_importances_):\n",
    "        idx_lst.append([feat, i])\n",
    "    return sorted(idx_lst, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.24660715000309424, 0],\n",
       " [0.11880706413405626, 5],\n",
       " [0.11152332641382312, 9],\n",
       " [0.059908103854401686, 3],\n",
       " [0.05645230632134722, 4],\n",
       " [0.047170463008234624, 1],\n",
       " [0.041989991641566075, 7],\n",
       " [0.041393662003483399, 6],\n",
       " [0.040573691144154225, 8],\n",
       " [0.03374382486116325, 2],\n",
       " [0.022282472125462558, 13],\n",
       " [0.016397530318239713, 35],\n",
       " [0.013449267507324187, 17],\n",
       " [0.013250193937924695, 23],\n",
       " [0.011661033568579889, 12],\n",
       " [0.010540464318108169, 15],\n",
       " [0.010170187339738216, 36],\n",
       " [0.010121195893610394, 25],\n",
       " [0.0096949731577236822, 10],\n",
       " [0.0091872477096017341, 52],\n",
       " [0.0089744993140728011, 51],\n",
       " [0.006788750888043503, 11],\n",
       " [0.0055391919733450423, 45],\n",
       " [0.0054293017962487528, 53],\n",
       " [0.0053588437574285715, 19],\n",
       " [0.0049212520631274469, 42],\n",
       " [0.0047951413589761554, 46],\n",
       " [0.0041894587820109786, 26],\n",
       " [0.0040481499729057464, 37],\n",
       " [0.0040246374942260969, 44],\n",
       " [0.0031794358396381893, 43],\n",
       " [0.0022974648204542028, 16],\n",
       " [0.0020622543592601233, 24],\n",
       " [0.0019328281663092354, 48],\n",
       " [0.0018229078709950189, 33],\n",
       " [0.0016653287563386, 30],\n",
       " [0.0016444338163877404, 14],\n",
       " [0.00087358872976643306, 32],\n",
       " [0.00086695437039335083, 29],\n",
       " [0.00073455187202405708, 34],\n",
       " [0.00064538854267874981, 40],\n",
       " [0.00063984919724873628, 50],\n",
       " [0.00061868426918536409, 18],\n",
       " [0.00056286891815842874, 47],\n",
       " [0.00034229307678299877, 39],\n",
       " [0.00031240009684501021, 27],\n",
       " [0.0002641289528231494, 38],\n",
       " [0.00016403733873380822, 41],\n",
       " [0.00012673478571201076, 49],\n",
       " [0.00010551968848467165, 31],\n",
       " [9.3768488960160678e-05, 22],\n",
       " [3.5761473959934998e-05, 21],\n",
       " [1.335644975974367e-05, 20],\n",
       " [2.0834570778478678e-06, 28]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf100 = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=1)\n",
    "clf100.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95633541014078693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No surprise, more trees, more accurate \n",
    "clf100.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  25.4s\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.5s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.3s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  33.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 4.1min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  26.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.1s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.8min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  32.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.8s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.2s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  22.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  24.7s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  23.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  44.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.8s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  37.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 15.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  36.4s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.2min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.4min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  32.6s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  28.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  26.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  29.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.3s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.8s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.2s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.4s\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 24.8min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_depth=np.arange(2,10),\n",
    "                  max_features=np.arange(7, 14),\n",
    "                  max_leaf_nodes=np.arange(2,8)\n",
    "                 )\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=13, max_leaf_nodes=7,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 13, 'max_leaf_nodes': 7, 'n_estimators': 100}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67040599720793248"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.1s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.3s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.2s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.1s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.0s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 5.8min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 6.4min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 6.0min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  57.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  55.2s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  50.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  16.9s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  17.4s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.1s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  14.0s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=  10.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.4s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.3s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 17.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. n_estimators=25, max_features=13, total=  16.0s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  15.6s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.2s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  18.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.4s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.2s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  12.9s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.2s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.1s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  21.5s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.5min\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.5s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  18.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 19.0min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_features=np.arange(7, 14)\n",
    "                 )\n",
    "\n",
    "grid_stump = RandomizedSearchCV(estimator=RandomForestClassifier(max_depth=1),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid_stump = grid_stump.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=1, max_features=13, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 13, 'n_estimators': 100}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59520758830391463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "                                 # use in the _make_split method\n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "\n",
    "\n",
    "        # This piece of code is used to provide feature names to the Decision tree\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            # if the user has not provided feature names, just give them numbers\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            # otherwise, these are the names\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # * Create True/False array of whether the variable is categorical\n",
    "        # use a lambda function called is_categorical to determine if the variable is an instance\n",
    "        # of str, bool or unicode - in that case is_categorical will be true\n",
    "        # otherwise False. Look up the function isinstance()\n",
    "\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) \n",
    "            \n",
    "        # Each variable (organized by index) is given a label categorical or not\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        # Call the build_tree function\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "\n",
    "        #  * initialize a root TreeNode\n",
    "        node = TreeNode()\n",
    "        # * set index, value, splits as the output of self._choose_split_index(X,y)\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        # if no index is returned from the split index or we cannot split\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            # * set the node to be a leaf\n",
    "            node.leaf = True\n",
    "            # * set the classes attribute to the number of classes\n",
    "            # * we have in this leaf with Counter()\n",
    "            node.classes = Counter(y)\n",
    "            # * set the name of the node to be the most common class in it\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "\n",
    "        else: # otherwise we can split (again this comes out of choose_split_index\n",
    "            # * set X1, y1, X2, y2 to be the splits\n",
    "            X1, y1, X2, y2 = splits\n",
    "            # * the node column should be set to the index coming from split_index\n",
    "            node.column = index\n",
    "            # * the node name is the feature name as determined by\n",
    "            #   the index (column name)\n",
    "            node.name = self.feature_names[index]\n",
    "\n",
    "            # * set the node value to be the value of the split\n",
    "            node.value = value\n",
    "\n",
    "            # * set the categorical flag of the node to be the category of the column\n",
    "            node.categorical = self.categorical[index]\n",
    "\n",
    "            # * now continue recursing down both branches of the split\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the total length of y. This is the p(C)\n",
    "            # * add the entropy p(C) ln p(C) to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C * np.log(p_C)\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the size of y. This is the p(C)\n",
    "            # * add p(C)**2 to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C**2\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "\n",
    "        # * slice the split column from X with the split_index\n",
    "        split_column = X[:, split_index]\n",
    "        # * if the variable of this column is categorical\n",
    "        if self.categorical[split_index]:\n",
    "            # * select the indices of the rows in the column\n",
    "            #  with the split_value (T/F) into one set of indices (call them A)\n",
    "            A = split_column == split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            # that don't have the split_value into another\n",
    "            #  set of indices (call them B)\n",
    "            B = split_column != split_value\n",
    "        # * else if the variable is not categorical\n",
    "        else:\n",
    "             # * select the indices of the rows in the column\n",
    "            #  less than the split value into one set of indices (call them A)\n",
    "            A = split_column < split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            #  greater or equal to  the split value into\n",
    "            # another set of indices (call them B)\n",
    "            B = split_column >= split_value\n",
    "            \n",
    "        return X[A], y[A], X[B], y[B]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "        # * set total equal to the impurity_criterion\n",
    "        total = self.impurity_criterion(y)\n",
    "        \n",
    "        e2 = len(y1)/len(y)*self.impurity_criterion(y1) + len(y2)/len(y)*self.impurity_criterion(y2)\n",
    "        total -= e2\n",
    "#         # * for each of the possible splits y1 and y2\n",
    "#         for split in  \n",
    "#             # * calculate the impurity_criterion of the split\n",
    "#             imp_cri = self.impurity_criterion(split) \n",
    "#             # * subtract this value from the total, multiplied by split_size/y_size\n",
    "#             total -= imp_cri * len(split)/\n",
    "            \n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "        X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "\n",
    "        # set these initial variables to None\n",
    "        split_index, split_value, split = None, None, None\n",
    "        # we need to keep track of the maximum entropic gain\n",
    "        max_gain = 0\n",
    "\n",
    "        # * for each column in X\n",
    "        for col in range(X.shape[1]):\n",
    "            # * set an array called values to be the\n",
    "            # unique values in that column (use np.unique)\n",
    "            values = np.unique(X[:, col])\n",
    "            # if there are less than 2 values, move on to the next column\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "\n",
    "            # * for each value V in the values array\n",
    "            for val in values:\n",
    "                # * make a temporary split (using the column index and V) with make_split\n",
    "                temporary_split = self._make_split(X, y, col, val)\n",
    "                # * calculate the information gain between the original y, y1 and y2\n",
    "                X1, y1, X2, y2 = temporary_split\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                # * if this gain is greater than the max_gain\n",
    "                if gain > max_gain:\n",
    "\n",
    "                    # * set max_gain, split_index, and split_value to be equal\n",
    "                    # to the current max_gain, column and value\n",
    "                    # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "                    split = temporary_split\n",
    "                    max_gain, split_index, split_value = gain, col, val\n",
    "                   \n",
    "        return split_index, split_value, split \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree. This will allow you to $:print tree\n",
    "        '''\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    '''A Random Forest class'''\n",
    "\n",
    "    def __init__(self, num_trees, num_features):\n",
    "        '''\n",
    "           num_trees:  number of trees to create in the forest:\n",
    "        num_features:  the number of features to consider when choosing the\n",
    "                           best split for each node of the decision trees\n",
    "        '''\n",
    "        self.num_trees = num_trees\n",
    "        self.num_features = num_features\n",
    "        self.forest = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X:  two dimensional numpy array representing feature matrix\n",
    "                for test data\n",
    "        y:  numpy array representing labels for test data\n",
    "        '''\n",
    "        self.forest = self.build_forest(X, y, self.num_trees, X.shape[0], \\\n",
    "                                        self.num_features)\n",
    "\n",
    "    def build_forest(self, X, y, num_trees, num_samples, num_features):\n",
    "\n",
    "        # * Return a list of num_trees DecisionTrees.\n",
    "        row, col = X.shape\n",
    "        forest = []\n",
    "        \n",
    "        for tree in range(num_trees):\n",
    "            # create a random set of X_samples with replacement\n",
    "            X_samp = np.random.randint(row, size=num_samples)\n",
    "            # create a random permutation of features (list)[sliced to length num_features]\n",
    "            y_samp = np.random.permutation(col)[:num_features]\n",
    "            X_tree = X[X_samp,:][:,y_samp]\n",
    "            y_tree = y[X_samp]\n",
    "            tree = DecisionTree()\n",
    "            tree.fit(X_tree, y_tree, feature_names=y_samp)\n",
    "            forest.append(tree)\n",
    "            \n",
    "        return forest\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        '''\n",
    "        Return a numpy array of the labels predicted for the given test data.\n",
    "        '''\n",
    "       \n",
    "        # * Each one of the trees is allowed to predict on the same row of input data. The majority vote\n",
    "        # is the output of the whole forest. This becomes a single prediction.\n",
    "        \n",
    "        predictions  = []\n",
    "        for tree in self.forest:\n",
    "            predict = tree.predict(X[:,tree.feature_names])\n",
    "            predictions.append(predict)\n",
    "        # thank you Tristan! Count along the columns and find the most common    \n",
    "        return np.array(np.apply_along_axis(lambda col: Counter(col).most_common()[0][0], arr=predictions, axis=0))\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "\n",
    "        '''\n",
    "        Return the accuracy of the Random Forest for the given test data.\n",
    "        '''\n",
    "\n",
    "        # * In this case you simply compute the accuracy formula as we have defined in class. Compare predicted y to\n",
    "        # the actual input y.\n",
    "        \n",
    "        return sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
