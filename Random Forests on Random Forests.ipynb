{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_covtype # dataset\n",
    "from sklearn.model_selection import train_test_split # split dataset into training/test sets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the dataset from:\n",
    "# \"http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\"\n",
    "cover_type = fetch_covtype() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Forest covertype dataset.\\n\\nA classic dataset for classification benchmarks, featuring categorical and\\nreal-valued features.\\n\\nThe dataset page is available from UCI Machine Learning Repository\\n\\n    http://archive.ics.uci.edu/ml/datasets/Covertype\\n\\nCourtesy of Jock A. Blackard and Colorado State University.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_type.DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 2, ..., 3, 3, 3], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_type.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from the Forest_Cover_Type.ipynb data exploration we discovered there are 7 distinct cover_types\n",
    "# set these covertypes as our target, y \n",
    "y = cover_type.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(581012, 54)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_type.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our data contains 54 features. Explored in depth within the Forest_Cover_Type.ipynb\n",
    "# set this 581012 x 54 matrix as our feature matrix, X\n",
    "X = cover_type.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# since our dataset is rather large, and we will be doing cross validation on our training set,\n",
    "# we set the train_size parameter to be 90% and set aside %10 to test on\n",
    "# due to large class imbalances in our target matrix, we\n",
    "# set the stratify parameter=y. this makes a split so that the proportion of classes in the \n",
    "# test and train sets will be similar\n",
    "X_train, X_test, y_train, y_test=\\\n",
    "                            train_test_split(X, y, train_size=.90, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(522910, 54) (58102, 54)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting our training data with scikit learn's RandomForestClassifier\n",
    "# Choosing this algorithm over others due to our feature space. \n",
    "# 44 of our features are binary, whether or not the tree is in one of 4 wilderness areas\n",
    "# and whether or not the tree is found in one of 40 soil types\n",
    "# In the Forest_Cover_Type.ipynb, the goal was to correctly classify only one tree type (7)\n",
    "# Through data exploration, and the logit function it was found that nearly all of the \n",
    "# features were statistically significant. \n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9441155209803449"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using only 10 trees, we are able to predict with a very high accuracy\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=95787995, splitter='best')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.estimators_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def srted_index():\n",
    "    idx_lst = []\n",
    "    for i, feat in enumerate(clf.feature_importances_):\n",
    "        idx_lst.append([feat, i])\n",
    "    return sorted(idx_lst, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.24660715000309424, 0],\n",
       " [0.11880706413405626, 5],\n",
       " [0.11152332641382312, 9],\n",
       " [0.059908103854401686, 3],\n",
       " [0.05645230632134722, 4],\n",
       " [0.047170463008234624, 1],\n",
       " [0.041989991641566075, 7],\n",
       " [0.041393662003483399, 6],\n",
       " [0.040573691144154225, 8],\n",
       " [0.03374382486116325, 2],\n",
       " [0.022282472125462558, 13],\n",
       " [0.016397530318239713, 35],\n",
       " [0.013449267507324187, 17],\n",
       " [0.013250193937924695, 23],\n",
       " [0.011661033568579889, 12],\n",
       " [0.010540464318108169, 15],\n",
       " [0.010170187339738216, 36],\n",
       " [0.010121195893610394, 25],\n",
       " [0.0096949731577236822, 10],\n",
       " [0.0091872477096017341, 52],\n",
       " [0.0089744993140728011, 51],\n",
       " [0.006788750888043503, 11],\n",
       " [0.0055391919733450423, 45],\n",
       " [0.0054293017962487528, 53],\n",
       " [0.0053588437574285715, 19],\n",
       " [0.0049212520631274469, 42],\n",
       " [0.0047951413589761554, 46],\n",
       " [0.0041894587820109786, 26],\n",
       " [0.0040481499729057464, 37],\n",
       " [0.0040246374942260969, 44],\n",
       " [0.0031794358396381893, 43],\n",
       " [0.0022974648204542028, 16],\n",
       " [0.0020622543592601233, 24],\n",
       " [0.0019328281663092354, 48],\n",
       " [0.0018229078709950189, 33],\n",
       " [0.0016653287563386, 30],\n",
       " [0.0016444338163877404, 14],\n",
       " [0.00087358872976643306, 32],\n",
       " [0.00086695437039335083, 29],\n",
       " [0.00073455187202405708, 34],\n",
       " [0.00064538854267874981, 40],\n",
       " [0.00063984919724873628, 50],\n",
       " [0.00061868426918536409, 18],\n",
       " [0.00056286891815842874, 47],\n",
       " [0.00034229307678299877, 39],\n",
       " [0.00031240009684501021, 27],\n",
       " [0.0002641289528231494, 38],\n",
       " [0.00016403733873380822, 41],\n",
       " [0.00012673478571201076, 49],\n",
       " [0.00010551968848467165, 31],\n",
       " [9.3768488960160678e-05, 22],\n",
       " [3.5761473959934998e-05, 21],\n",
       " [1.335644975974367e-05, 20],\n",
       " [2.0834570778478678e-06, 28]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srted_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   48.2s\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=-1, oob_score=False,\n",
       "            random_state=None, verbose=1, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf100 = RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=1)\n",
    "clf100.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    2.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.95633541014078693"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No surprise, more trees, more accurate \n",
    "clf100.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  25.4s\n",
      "[CV] n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.5s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  27.3s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=7, max_features=7, max_depth=6, total=  33.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.7min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 4.1min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  26.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.1s\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=200, max_leaf_nodes=6, max_features=9, max_depth=4, total= 3.8min\n",
      "[CV] n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7 .\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=8, max_depth=6, total= 2.0min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  32.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=4, max_features=11, max_depth=7, total=  29.8s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2 ..\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  20.2s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  22.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  24.7s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=9, max_depth=2, total=  23.6s\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  44.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.8s\n",
      "[CV] n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  38.6s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  37.2s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 15.3min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=50, max_leaf_nodes=4, max_features=7, max_depth=7, total=  36.4s\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=100, max_leaf_nodes=7, max_features=13, max_depth=6, total= 2.8min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.2min\n",
      "[CV] n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8 \n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.4min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  32.6s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  28.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  26.7s\n",
      "[CV] n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6 ..\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 5.5min\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  29.9s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=50, max_leaf_nodes=2, max_features=9, max_depth=6, total=  27.2s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.3s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.5s\n",
      "[CV] n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5 .\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  27.8s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.2s\n",
      "[CV]  n_estimators=25, max_leaf_nodes=5, max_features=11, max_depth=5, total=  25.4s\n",
      "[CV]  n_estimators=200, max_leaf_nodes=7, max_features=12, max_depth=8, total= 3.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 24.8min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_depth=np.arange(2,10),\n",
    "                  max_features=np.arange(7, 14),\n",
    "                  max_leaf_nodes=np.arange(2,8)\n",
    "                 )\n",
    "\n",
    "grid = RandomizedSearchCV(estimator=RandomForestClassifier(),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid = grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=13, max_leaf_nodes=7,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 13, 'max_leaf_nodes': 7, 'n_estimators': 100}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67040599720793248"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=200, max_features=10 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 1.2min\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.1s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.3s\n",
      "[CV] n_estimators=50, max_features=13 ................................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  21.2s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.1s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................. n_estimators=50, max_features=13, total=  20.0s\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 5.8min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=10, total= 6.4min\n",
      "[CV] n_estimators=100, max_features=13 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total= 6.0min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  57.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  55.2s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=100, max_features=13, total=  50.3s\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=200, max_features=12 ...............................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  16.9s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  17.4s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.1s\n",
      "[CV] n_estimators=25, max_features=10 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  14.0s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] ................. n_estimators=25, max_features=10, total=  12.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=  10.9s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=25, max_features=9 .................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.4s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.3s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] .................. n_estimators=25, max_features=9, total=   9.7s\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................ n_estimators=200, max_features=12, total= 1.8min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=200, max_features=9 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.3min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.4min\n",
      "[CV] n_estimators=25, max_features=13 ................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 17.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ................. n_estimators=25, max_features=13, total=  16.0s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  15.6s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.2s\n",
      "[CV] n_estimators=25, max_features=13 ................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 2.0min\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  18.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=13, total=  16.4s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.2s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  13.5s\n",
      "[CV] n_estimators=25, max_features=12 ................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  12.9s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.2s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=25, max_features=12, total=  11.1s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  21.5s\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] ................. n_estimators=200, max_features=9, total= 1.5min\n",
      "[CV] n_estimators=50, max_features=8 .................................\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.5s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  28.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=  18.7s\n",
      "[CV] .................. n_estimators=50, max_features=8, total=   9.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 19.0min finished\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(n_estimators=[25, 50, 100, 200],\n",
    "                  max_features=np.arange(7, 14)\n",
    "                 )\n",
    "\n",
    "grid_stump = RandomizedSearchCV(estimator=RandomForestClassifier(max_depth=1),\n",
    "                         n_iter=10,\n",
    "                         param_distributions=param_grid,\n",
    "                         cv=5,\n",
    "                         n_jobs=-1,\n",
    "                         verbose=2)\n",
    "\n",
    "grid_stump = grid_stump.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=1, max_features=13, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 13, 'n_estimators': 100}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59520758830391463"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_stump.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TreeNode(object):\n",
    "    '''\n",
    "    A node class for a decision tree.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.column = None  # (int)    index of feature to split on\n",
    "        self.value = None  # value of the feature to split on\n",
    "        self.categorical = True  # (bool) whether or not node is split on\n",
    "                                 # categorial feature\n",
    "        self.name = None    # (string) name of feature (or name of class in the\n",
    "                            #          case of a list)\n",
    "        self.left = None    # (TreeNode) left child\n",
    "        self.right = None   # (TreeNode) right child\n",
    "        self.leaf = False   # (bool)   true if node is a leaf, false otherwise\n",
    "        self.classes = Counter()  # (Counter) only necessary for leaf node:\n",
    "                                  #           key is class name and value is\n",
    "                                  #           count of the count of data points\n",
    "                                  #           that terminate at this leaf\n",
    "\n",
    "    def predict_one(self, x):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - x: 1d numpy array (single data point)\n",
    "        OUTPUT:\n",
    "            - y: predicted label\n",
    "        Return the predicted label for a single data point.\n",
    "        '''\n",
    "        if self.leaf:\n",
    "            return self.name\n",
    "        col_value = x[self.column]\n",
    "\n",
    "        if self.categorical:\n",
    "            if col_value == self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)\n",
    "        else:\n",
    "            if col_value < self.value:\n",
    "                return self.left.predict_one(x)\n",
    "            else:\n",
    "                return self.right.predict_one(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    '''\n",
    "    A decision tree class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, impurity_criterion='entropy'):\n",
    "        '''\n",
    "        Initialize an empty DecisionTree.\n",
    "        '''\n",
    "\n",
    "        self.root = None  # root Node\n",
    "        self.feature_names = None  # string names of features (for interpreting\n",
    "                                   # the tree)\n",
    "        self.categorical = None  # Boolean array of whether variable is\n",
    "                                 # categorical (or continuous)\n",
    "                                 # use in the _make_split method\n",
    "        self.impurity_criterion = self._entropy \\\n",
    "                                  if impurity_criterion == 'entropy' \\\n",
    "                                  else self._gini\n",
    "\n",
    "    def fit(self, X, y, feature_names=None):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - feature_names: numpy array of strings\n",
    "        OUTPUT: None\n",
    "        Build the decision tree.\n",
    "        X is a 2 dimensional array with each column being a feature and each\n",
    "        row a data point.\n",
    "        y is a 1 dimensional array with each value being the corresponding\n",
    "        label.\n",
    "        feature_names is an optional list containing the names of each of the\n",
    "        features.\n",
    "        '''\n",
    "\n",
    "\n",
    "        # This piece of code is used to provide feature names to the Decision tree\n",
    "        if feature_names is None or len(feature_names) != X.shape[1]:\n",
    "            # if the user has not provided feature names, just give them numbers\n",
    "            self.feature_names = np.arange(X.shape[1])\n",
    "        else:\n",
    "            # otherwise, these are the names\n",
    "            self.feature_names = feature_names\n",
    "\n",
    "        # * Create True/False array of whether the variable is categorical\n",
    "        # use a lambda function called is_categorical to determine if the variable is an instance\n",
    "        # of str, bool or unicode - in that case is_categorical will be true\n",
    "        # otherwise False. Look up the function isinstance()\n",
    "\n",
    "        is_categorical = lambda x: isinstance(x, str) or \\\n",
    "                                   isinstance(x, bool) \n",
    "            \n",
    "        # Each variable (organized by index) is given a label categorical or not\n",
    "        self.categorical = np.vectorize(is_categorical)(X[0])\n",
    "\n",
    "        # Call the build_tree function\n",
    "        self.root = self._build_tree(X, y)\n",
    "\n",
    "    def _build_tree(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - TreeNode\n",
    "        Recursively build the decision tree. Return the root node.\n",
    "        '''\n",
    "\n",
    "        #  * initialize a root TreeNode\n",
    "        node = TreeNode()\n",
    "        # * set index, value, splits as the output of self._choose_split_index(X,y)\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "\n",
    "        # if no index is returned from the split index or we cannot split\n",
    "        if index is None or len(np.unique(y)) == 1:\n",
    "            # * set the node to be a leaf\n",
    "            node.leaf = True\n",
    "            # * set the classes attribute to the number of classes\n",
    "            # * we have in this leaf with Counter()\n",
    "            node.classes = Counter(y)\n",
    "            # * set the name of the node to be the most common class in it\n",
    "            node.name = node.classes.most_common(1)[0][0]\n",
    "\n",
    "        else: # otherwise we can split (again this comes out of choose_split_index\n",
    "            # * set X1, y1, X2, y2 to be the splits\n",
    "            X1, y1, X2, y2 = splits\n",
    "            # * the node column should be set to the index coming from split_index\n",
    "            node.column = index\n",
    "            # * the node name is the feature name as determined by\n",
    "            #   the index (column name)\n",
    "            node.name = self.feature_names[index]\n",
    "\n",
    "            # * set the node value to be the value of the split\n",
    "            node.value = value\n",
    "\n",
    "            # * set the categorical flag of the node to be the category of the column\n",
    "            node.categorical = self.categorical[index]\n",
    "\n",
    "            # * now continue recursing down both branches of the split\n",
    "            node.left = self._build_tree(X1, y1)\n",
    "            node.right = self._build_tree(X2, y2)\n",
    "\n",
    "        return node\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the entropy of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the total length of y. This is the p(C)\n",
    "            # * add the entropy p(C) ln p(C) to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C * np.log(p_C)\n",
    "        return -total\n",
    "\n",
    "    def _gini(self, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the gini impurity of the array y.\n",
    "        '''\n",
    "\n",
    "        total = 0\n",
    "        # * for each unique class C in y\n",
    "        for c in np.unique(y):\n",
    "            # * count up the number of times the class C appears and divide by\n",
    "            # * the size of y. This is the p(C)\n",
    "            # * add p(C)**2 to the total\n",
    "            p_C = np.sum(y == c) / float(len(y))\n",
    "            total += p_C**2\n",
    "        return 1 - total\n",
    "\n",
    "    def _make_split(self, X, y, split_index, split_value):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "            - split_index: int (index of feature)\n",
    "            - split_value: int/float/bool/str (value of feature)\n",
    "        OUTPUT:\n",
    "            - X1: 2d numpy array (feature matrix for subset 1)\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - X2: 2d numpy array (feature matrix for subset 2)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        Return the two subsets of the dataset achieved by the given feature and\n",
    "        value to split on.\n",
    "        Call the method like this:\n",
    "        X1, y1, X2, y2 = self._make_split(X, y, split_index, split_value)\n",
    "        X1, y1 is a subset of the data.\n",
    "        X2, y2 is the other subset of the data.\n",
    "        '''\n",
    "\n",
    "        # * slice the split column from X with the split_index\n",
    "        split_column = X[:, split_index]\n",
    "        # * if the variable of this column is categorical\n",
    "        if self.categorical[split_index]:\n",
    "            # * select the indices of the rows in the column\n",
    "            #  with the split_value (T/F) into one set of indices (call them A)\n",
    "            A = split_column == split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            # that don't have the split_value into another\n",
    "            #  set of indices (call them B)\n",
    "            B = split_column != split_value\n",
    "        # * else if the variable is not categorical\n",
    "        else:\n",
    "             # * select the indices of the rows in the column\n",
    "            #  less than the split value into one set of indices (call them A)\n",
    "            A = split_column < split_value\n",
    "            # * select the indices of the rows in the column\n",
    "            #  greater or equal to  the split value into\n",
    "            # another set of indices (call them B)\n",
    "            B = split_column >= split_value\n",
    "            \n",
    "        return X[A], y[A], X[B], y[B]\n",
    "\n",
    "    def _information_gain(self, y, y1, y2):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - y: 1d numpy array\n",
    "            - y1: 1d numpy array (labels for subset 1)\n",
    "            - y2: 1d numpy array (labels for subset 2)\n",
    "        OUTPUT:\n",
    "            - float\n",
    "        Return the information gain of making the given split.\n",
    "        Use self.impurity_criterion(y) rather than calling _entropy or _gini\n",
    "        directly.\n",
    "        '''\n",
    "        # * set total equal to the impurity_criterion\n",
    "        total = self.impurity_criterion(y)\n",
    "        \n",
    "        e2 = len(y1)/len(y)*self.impurity_criterion(y1) + len(y2)/len(y)*self.impurity_criterion(y2)\n",
    "        total -= e2\n",
    "#         # * for each of the possible splits y1 and y2\n",
    "#         for split in  \n",
    "#             # * calculate the impurity_criterion of the split\n",
    "#             imp_cri = self.impurity_criterion(split) \n",
    "#             # * subtract this value from the total, multiplied by split_size/y_size\n",
    "#             total -= imp_cri * len(split)/\n",
    "            \n",
    "        return total\n",
    "\n",
    "    def _choose_split_index(self, X, y):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "            - y: 1d numpy array\n",
    "        OUTPUT:\n",
    "            - index: int (index of feature)\n",
    "            - value: int/float/bool/str (value of feature)\n",
    "            - splits: (2d array, 1d array, 2d array, 1d array)\n",
    "        Determine which feature and value to split on. Return the index and\n",
    "        value of the optimal split along with the split of the dataset.\n",
    "        Return None, None, None if there is no split which improves information\n",
    "        gain.\n",
    "        Call the method like this:\n",
    "        index, value, splits = self._choose_split_index(X, y)\n",
    "        X1, y1, X2, y2 = splits\n",
    "        '''\n",
    "\n",
    "        # set these initial variables to None\n",
    "        split_index, split_value, split = None, None, None\n",
    "        # we need to keep track of the maximum entropic gain\n",
    "        max_gain = 0\n",
    "\n",
    "        # * for each column in X\n",
    "        for col in range(X.shape[1]):\n",
    "            # * set an array called values to be the\n",
    "            # unique values in that column (use np.unique)\n",
    "            values = np.unique(X[:, col])\n",
    "            # if there are less than 2 values, move on to the next column\n",
    "            if len(values) < 2:\n",
    "                continue\n",
    "\n",
    "            # * for each value V in the values array\n",
    "            for val in values:\n",
    "                # * make a temporary split (using the column index and V) with make_split\n",
    "                temporary_split = self._make_split(X, y, col, val)\n",
    "                # * calculate the information gain between the original y, y1 and y2\n",
    "                X1, y1, X2, y2 = temporary_split\n",
    "                gain = self._information_gain(y, y1, y2)\n",
    "                # * if this gain is greater than the max_gain\n",
    "                if gain > max_gain:\n",
    "\n",
    "                    # * set max_gain, split_index, and split_value to be equal\n",
    "                    # to the current max_gain, column and value\n",
    "                    # * set the output splits to the current split setup (X1, y1, X2, y2)\n",
    "                    split = temporary_split\n",
    "                    max_gain, split_index, split_value = gain, col, val\n",
    "                   \n",
    "        return split_index, split_value, split \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        INPUT:\n",
    "            - X: 2d numpy array\n",
    "        OUTPUT:\n",
    "            - y: 1d numpy array\n",
    "        Return an array of predictions for the feature matrix X.\n",
    "        '''\n",
    "\n",
    "        return np.apply_along_axis(self.root.predict_one, axis=1, arr=X)\n",
    "\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        Return string representation of the Decision Tree. This will allow you to $:print tree\n",
    "        '''\n",
    "        return str(self.root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomForest(object):\n",
    "    '''A Random Forest class'''\n",
    "\n",
    "    def __init__(self, num_trees, num_features):\n",
    "        '''\n",
    "           num_trees:  number of trees to create in the forest:\n",
    "        num_features:  the number of features to consider when choosing the\n",
    "                           best split for each node of the decision trees\n",
    "        '''\n",
    "        self.num_trees = num_trees\n",
    "        self.num_features = num_features\n",
    "        self.forest = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        X:  two dimensional numpy array representing feature matrix\n",
    "                for test data\n",
    "        y:  numpy array representing labels for test data\n",
    "        '''\n",
    "        self.forest = self.build_forest(X, y, self.num_trees, X.shape[0], \\\n",
    "                                        self.num_features)\n",
    "\n",
    "    def build_forest(self, X, y, num_trees, num_samples, num_features):\n",
    "\n",
    "        # * Return a list of num_trees DecisionTrees.\n",
    "        row, col = X.shape\n",
    "        forest = []\n",
    "        \n",
    "        for tree in range(num_trees):\n",
    "            # create a random set of X_samples with replacement\n",
    "            X_samp = np.random.randint(row, size=num_samples)\n",
    "            # create a random permutation of features (list)[sliced to length num_features]\n",
    "            y_samp = np.random.permutation(col)[:num_features]\n",
    "            X_tree = X[X_samp,:][:,y_samp]\n",
    "            y_tree = y[X_samp]\n",
    "            tree = DecisionTree()\n",
    "            tree.fit(X_tree, y_tree, feature_names=y_samp)\n",
    "            forest.append(tree)\n",
    "            \n",
    "        return forest\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        '''\n",
    "        Return a numpy array of the labels predicted for the given test data.\n",
    "        '''\n",
    "       \n",
    "        # * Each one of the trees is allowed to predict on the same row of input data. The majority vote\n",
    "        # is the output of the whole forest. This becomes a single prediction.\n",
    "        \n",
    "        predictions  = []\n",
    "        for tree in self.forest:\n",
    "            predict = tree.predict(X[:,tree.feature_names])\n",
    "            predictions.append(predict)\n",
    "        # thank you Tristan! Count along the columns and find the most common    \n",
    "        return np.array(np.apply_along_axis(lambda col: Counter(col).most_common()[0][0], arr=predictions, axis=0))\n",
    "\n",
    "    \n",
    "    def score(self, X, y):\n",
    "\n",
    "        '''\n",
    "        Return the accuracy of the Random Forest for the given test data.\n",
    "        '''\n",
    "\n",
    "        # * In this case you simply compute the accuracy formula as we have defined in class. Compare predicted y to\n",
    "        # the actual input y.\n",
    "        \n",
    "        return sum(self.predict(X) == y) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForest(10,7)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.55311349006918864"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
